{
  
    
        "post0": {
            "title": "Logistic Regression",
            "content": "Content prepared by: Berk Hakbilen . Logistic Regression Theory . Logistic Regression for Classification . Despite its name, logistic regression is a essential technique for binary classification and multiclass classification rather than a regression algorithm. It falls under the category of linear classifiers. It is a fast, and simple model, making it easier to interpret the results. . Logistic regression is like its name suggests also a regression analysis. However, unlike linear regression (which is not suitable for a classification analysis), the calculated result is the probability of an event. . Let&#39;s have a look at the linear regression model and then derive the logistic regression function from this. . Linear regression formula: $$y^i = a_0 + a_1x_1^i + .... + a_nx_n^i $$ . On the other hand the so called sigmoid function is: $$P(x) = frac{1}{1 + exp(-x)} $$ . and its curve: . Here we can see that its output is always a value ranging from 0 to 1 which is the exact behaviour we want for a binary classification. . Like we just mentioned, for a classification problem, we want to get probabilities between 0 - 1 as results. We can achieve that by substituting our linear regression function into our sigmoid function, we obtain our logistic regression function: $$P(y^i) = frac{1}{1 + exp(-(a_0 + a_1x_1^i + .... + a_nx_n^i))} $$ . Looking at the formula we can see that the regression coefficients are now superscript of the exponential term (e). This way the regression coefficients from the linear regression function still effect the probability outcome of the logistic function. . Exploratory Data Analysis . from sklearn.datasets import load_breast_cancer import pandas as pd cancer = load_breast_cancer() df = pd.DataFrame(cancer.data, columns=cancer.feature_names) df[&#39;result&#39;] = pd.Series(cancer.target) . df.head() . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension radius error texture error perimeter error area error smoothness error compactness error concavity error concave points error symmetry error fractal dimension error worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension result . 0 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | 1.0950 | 0.9053 | 8.589 | 153.40 | 0.006399 | 0.04904 | 0.05373 | 0.01587 | 0.03003 | 0.006193 | 25.38 | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | 0 | . 1 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | 0.5435 | 0.7339 | 3.398 | 74.08 | 0.005225 | 0.01308 | 0.01860 | 0.01340 | 0.01389 | 0.003532 | 24.99 | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | 0 | . 2 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | 0.7456 | 0.7869 | 4.585 | 94.03 | 0.006150 | 0.04006 | 0.03832 | 0.02058 | 0.02250 | 0.004571 | 23.57 | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | 0 | . 3 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | 0.2597 | 0.09744 | 0.4956 | 1.1560 | 3.445 | 27.23 | 0.009110 | 0.07458 | 0.05661 | 0.01867 | 0.05963 | 0.009208 | 14.91 | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | 0 | . 4 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | 0.1809 | 0.05883 | 0.7572 | 0.7813 | 5.438 | 94.44 | 0.011490 | 0.02461 | 0.05688 | 0.01885 | 0.01756 | 0.005115 | 22.54 | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 mean radius 569 non-null float64 1 mean texture 569 non-null float64 2 mean perimeter 569 non-null float64 3 mean area 569 non-null float64 4 mean smoothness 569 non-null float64 5 mean compactness 569 non-null float64 6 mean concavity 569 non-null float64 7 mean concave points 569 non-null float64 8 mean symmetry 569 non-null float64 9 mean fractal dimension 569 non-null float64 10 radius error 569 non-null float64 11 texture error 569 non-null float64 12 perimeter error 569 non-null float64 13 area error 569 non-null float64 14 smoothness error 569 non-null float64 15 compactness error 569 non-null float64 16 concavity error 569 non-null float64 17 concave points error 569 non-null float64 18 symmetry error 569 non-null float64 19 fractal dimension error 569 non-null float64 20 worst radius 569 non-null float64 21 worst texture 569 non-null float64 22 worst perimeter 569 non-null float64 23 worst area 569 non-null float64 24 worst smoothness 569 non-null float64 25 worst compactness 569 non-null float64 26 worst concavity 569 non-null float64 27 worst concave points 569 non-null float64 28 worst symmetry 569 non-null float64 29 worst fractal dimension 569 non-null float64 30 result 569 non-null int64 dtypes: float64(30), int64(1) memory usage: 137.9 KB . import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) sns.countplot(df[&#39;result&#39;]) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd242d02950&gt; . cancer.target_names . array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;) . benign, malignant = df[&#39;result&#39;].value_counts() print(&#39;Number of benign results {} corresponding to {} percent: &#39;.format(benign,round(benign / len(df) * 100, 2))) print(&#39;Number of malignant results {} corresponding to {} percent: &#39;.format(malignant,round(malignant / len(df) * 100, 2))) . Number of benign results 357 corresponding to 62.74 percent: Number of malignant results 212 corresponding to 37.26 percent: . cols = [&#39;result&#39;, &#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;] sns.pairplot(data=df[cols], hue=&#39;result&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x7fd2423c4990&gt; . f, ax = plt.subplots(figsize=(20, 20)) corr = df.corr().round(2) # Create a mask for the lower triangle import numpy as np mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # create the heatmap sns.heatmap(corr, mask=mask, square=True, annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd2348f62d0&gt; . Logistic Regression Model . from sklearn.model_selection import train_test_split y = df[&#39;result&#39;].values X = df.drop(&#39;result&#39;,axis=1).values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . X_train.shape . (455, 30) . from sklearn.linear_model import LogisticRegression . model = LogisticRegression(max_iter=10000) . logisticregression = model.fit(X_train,y_train) . print(&quot;training set score: %f&quot; % logisticregression.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression.score(X_test, y_test)) . training set score: 0.960440 test set score: 0.956140 . Because our training and test score are closer to each other, we can see that we are actually underfitting. Let&#39;s try a higher C value which means a more complex model which fits better to the data. . model = LogisticRegression(max_iter=10000,C=1000) logisticregression_1000 = model.fit(X_train,y_train) print(&quot;training set score: %f&quot; % logisticregression_1000.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression_1000.score(X_test, y_test)) . training set score: 0.986813 test set score: 0.991228 . Because our training and test score are closer to each other, we can see that we are actually underfitting. Let&#39;s try a higher C value which means a more complex model which fits better to the data. . model = LogisticRegression(max_iter=10000,C=0.1) logisticregression_0_1 = model.fit(X_train,y_train) print(&quot;training set score: %f&quot; % logisticregression_0_1.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression_0_1.score(X_test, y_test)) . training set score: 0.949451 test set score: 0.964912 . A lower C does not create a big difference since our model is already underfitting to the data. . from sklearn.metrics import confusion_matrix y_pred = model.predict(X_test) cf_matrix = confusion_matrix(y_test, y_pred,labels=[0,1]) cf_matrix . array([[40, 3], [ 1, 70]]) . sns.heatmap(cf_matrix, annot=True, cmap=&#39;Blues&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f895589abd0&gt; . from sklearn.metrics import plot_confusion_matrix disp = plot_confusion_matrix(model, X_test, y_test, display_labels=[&#39;Benign&#39;,&#39;Malignant&#39;]) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator. warnings.warn(msg, category=FutureWarning) . from sklearn import metrics print(metrics.classification_report(y_test, y_pred, target_names=[&#39;Benign&#39;,&#39;Malignant&#39;])) . precision recall f1-score support Benign 0.98 0.93 0.95 43 Malignant 0.96 0.99 0.97 71 accuracy 0.96 114 macro avg 0.97 0.96 0.96 114 weighted avg 0.97 0.96 0.96 114 . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9649122807017544 Precision: 0.958904109589041 Recall: 0.9859154929577465 .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/11/09/Logistic-Regression.html",
            "relUrl": "/jupyter/2022/11/09/Logistic-Regression.html",
            "date": " • Nov 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Evaluation for Regression",
            "content": "Evaluation for Regression . Model evaluation is very important since we need to understand how well our model is performing. In comparison to classification, performance of a regression model is slightly harder to determine because, unlike classification, it is almost impossible to predict the exact value of a target variable. Therefore, we need a way to calculate how close our prediction value is to the real value. . There are different model evaluation metrics that are used popularly for regression models which we are going to dive into in the following sections. . Mean Absolute Error . Mean absolute error is a very intuitive and simple technique, therefore also popular. It is basically the average of the distances between the predicted and the true values. Basically the distances between the predicted and the real values are also the error terms. The overall error for the whole data is the average of all prediction error terms. We take the absolute of the distances/errors to prevent negative and positive terms/errors from cancelling each other. . $$ MAE = frac{1}{N} sum limits_{i=1}^{N}{|y_i - ŷ_i|} $$ . Advantages . MAE is not sensitive to outliers. Use MAE when you do not want outliers to play a big role in error calculated. | . Disadvantages . MAE is not differentiable globally. This is not convenient when we use it as a loss function, due to the gradient optimization method. | . Mean Squared Error (MSE) . MSE is one of widely used metrics for regression problems. MSE is the the measure of average of squared distance between the actual values and the predicted values. Squared terms help to also take into consideration of negative terms and avoid cancellation of the total error between positive and negative differences. . $$ MSE = frac{1}{N} sum limits_{i=1}^{N}{(y_i - ŷ_i)^2} $$ . Advantages . Graph of MSE is differantiable which means it can be easily used as a loss function. | MSE can be decomposed into variance and bias squared. This helps us understand the effect of variance or bias in data to the overall error. | . $$ MSE(ŷ) = Var(ŷ) + (Bias(ŷ))^2 $$ . Disadvantages . The value calculated MSE has a different unit than the target variable since it is squared. (Ex. meter --&gt; meter^2) | If there exists outliers in the data, then they are going to result in a larger error. Therefore, MSE is not robust to outliers (this can also be an advantage if you are looking to penalize outliers). | . Root Mean Squared Error (RMSE) . As the name already suggests, in RMSE we take the root of the mean of squared distances, meaning the root of MSE. RMSE is also a popularly used evaluation metric, especially in deep learning techniques. . $$ RMSE = sqrt{ frac{1}{N} sum limits_{i=1}^{N}{(y_i - ŷ_i)^2} }$$ . Advantages . The error calculated has the same unit as the target variables making the interpretation relatively easier. | . Disadvantages . Just like MSE, RMSE is also susceptible to outliers. | . R-Squared . R square is a different metric compared to the ones we have discussed until now. It does not directly measure the error of the model. . R-squared evaluates the scatter of the data points around the fitted regression line. It is the percentage of the target variable variation which the model considers compared to the actual target variable variance. It is also known as the &quot;coefficient of determination&quot; or goodness of fit. . $$ R^2 = frac{ text{Variance considered by model}}{ text{Total variance}} $$ . $$ R^2 = 1 - frac{SS_{regression}}{SS_{total}} = 1 - frac{ sum{(y_i - ŷ_i)^2}}{ sum{(y_i - y_{mean})^2}} $$ . As we can see above, R-squared is calculated by dividing the sum of squared error of predictions by the total sum of square, where predicted value is replaced by the mean of real values. . R-squared is always between 0 and 1. 0 indicates that the model does not explain any of the variation in the target variable around its mean value. The regression model basically predicts the mean of the parget variable. A value of 1 indicates, that the model explains all the variance in the target variable around its mean. . A larger R-squared value usually indicates that the regression model fits the data better. However, a high R-square model does not necessarily mean a good model. . import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression import seaborn as sns; sns.set_theme(color_codes=True) X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 15, random_state=42) plt.figure() ax = sns.regplot(x=X,y=y) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) . R-squared score: 0.977 . X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 200, random_state=42) plt.figure() ax = sns.regplot(x=X,y=y) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) . R-squared score: 0.294 . Advantages . R-square is a handy, and an intuitive metric of how well the model fits the data. Therefore, it is a good metric for a baseline model evaluation. However, due to the disadvantages we are going to discuss now, it should be used carefully. | . Disadvantages . R-squared can&#39;t determine if the predictions are biased, that is why looking at the residual plots in addition is a good idea. . | R-squared does not necessarily indicate that a regression model is good to go. It is also possible to have a low R-squared score for a good regression model and a high R-squared model for a bad model (especially due to overfitting). . | When new input variables (predictors) are added to the model, the R-square is going to increase (because we are adding more variance to the data) independent of an actual performance increase in model. It never decreases when new input variables are added. Therefore, a model with many input variables may seem to have a better performance jsut because it has more input variables. This is an issue which we are going to address with adjusted R-squared. . | . It is still possible to fit a good model to a dataset with a lot of variance which is likely going to have a low R-square. However, it does not necessarily mean the model is bad if it is still able to capture the general trend in the dataset, and capture the effect of change of a predictor on the target variables. R-square becomes a big problem when we want to predict a target variable with a high precision, meaning with a small prediction interval. . A high R-squared score also does not necessarily mean a good model because it is not able to detect the bias. Therefore, also checking the residual plots is a good idea. Like we mentioned previously, a model with a high R-squared score can also be overfitting since it captures most of the variance in the data. Therefore, it is always a good idea to check the R-squared score of the predictions from the model and compare it to the R-squared score from the training data. . Adjusted R-Squared . We previously mentioned that R-square score never actually decreases but increases when we add more input variables because we increase the variance in the data. To address this issue, we are going to talk about adjusted R-squared. . The adjusted R-sqaured is adjusted version of R-square where the number of input variables in the model are also considered. R-square can penalize the additional input variables given they do not contribute to the model performance. Let&#39;s have a look at how it is calculated. . $$ R_a^2 = 1- (( frac{n-1}{n-k-1})(1-R^2)) $$ . where: . $n = text{number of data points}$, $k= text{number of input variables}$ . As number of input variables/features increase, the denominator will decrease, R-squared will increase slightly or remain constant if the added features are not relevant, meaning the complete term in paranthesis is going to increase. The resultant adjusted R-squared score is going to decrease because we deducted the increasing term from 1. . However, if the added input variables are relevant, then the R-squared score will increase much more and the term (1-R2) will decrease a lot. When we subtract the complete term in paranthesis from 1, the overall adjusted R-squared score will increase. You see how the number of added variables and the amount of R-squared increase, help us adjust the R-squared score to account for new variables depending on if they are relevant or not. . def get_adj_r2(X,y): return 1 - (1-model.score(X, y))*(len(X) - 1) / (len(X) - (X.shape[1] - 1) - 1) . X, y = make_regression(n_samples = 80, n_features=4, n_informative=1, bias = 50, noise = 60, random_state=42) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) print(&#39;Adjusted R-squared score: {:.3f}&#39; .format(get_adj_r2(X,y))) . R-squared score: 0.582 Adjusted R-squared score: 0.566 . X, y = make_regression(n_samples = 80, n_features=4, n_informative=1, bias = 50, noise = 60, random_state=42) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) print(&#39;Adjusted R-squared score: {:.3f}&#39; .format(get_adj_r2(X,y))) . R-squared score: 0.582 Adjusted R-squared score: 0.566 .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/09/14/Evaluation-Regression.ipynb.html",
            "relUrl": "/jupyter/2022/09/14/Evaluation-Regression.ipynb.html",
            "date": " • Sep 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Evaluation Methods for Classification Problems",
            "content": "Confusion Matrix . Confusion Matrix is a table where we can clearly see the predicted categories and actual categories. The confusion matrix for a binary classification looks like this: . . True positives (TP): Predicted positive and are actually positive. . False positives (FP): Predicted positive and are actually negative. . True negatives (TN): Predicted negative and are actually negative. . False negatives (FN): Predicted negative and are actually positive. . To calculate the confusion matrix, we will be using the ConfusionMatrixDisplay class from the sklearn library. . Documentation for ConfusionMatrixDisplay . from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import ConfusionMatrixDisplay from sklearn.linear_model import LogisticRegression X,y = load_breast_cancer(return_X_y = True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = LogisticRegression(max_iter=10000) clf.fit(X_train, y_train) ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels = [&#39;malignant&#39;, &#39;benign&#39;]) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fc220ad5d10&gt; . Depending on some problems, it can be very important to see the number of false negatives or false positives. This is where the confusion matrix comes in very handy because it does exactly that. . In this case, we have the precision matrix for the cancer dataset. We can see that the number of true positives is 39. That means the model was able to identify 39 samples with cancer correctly. . However, we also have 1 false positive (the patient actually does not have cancer, but the model says the patient has cancer). False positives in this case may not be so critical, since the additional medical controls will later reveal that the patient does not have cancer (except for a few days of difficult days for the patient...). . What is here very critical is the 4 patients who were predicted as cancer-free but they do actually have cancer (false negatives). This is a case we do not want to have at all, since the lives of cancer patients are at stake here. Therefore, for this problem, we would aim for least possible number of false negatives. For another problem, reducing the number of false positives could be more important. This is why the confusion matrix is very important when it comes to classification problems. . Confusion Matrix for Multi-class Classification . The confusion matrix becomes larger as we move form a binary classification problem to a multi-class classification problem. This is how a confusion matrix for a multi-class problem would look like: . . The size of the confusion matrix is same as the number of target categories. One big advantage of confusion matrix is that, it allows us to see how many samples of which class were misclassified, and which predicted class was confused with which actual class. . Accuracy . Accuracy is a measure of how often the classifier makes a correct prediction. It is the ratio of total correct predictions to the total predictions. . $$ text{Accuracy} = frac{ text{TP + TN}}{TP + TN + FP + FN} $$ . Accuracy is a easy to understand metric and can be very useful when the dataset is balanced. However, it can be misleading when evaluating imbalanced datasets. When target classes in dataset have a large differences in terms of samples, it is an imbalanced dataset. Accuracy is not the best metric to use when dealing with imbalanced datasets. Let&#39;s have a look at the example below. . . When we look at the confusion matrix carefully, we see that we have only 25 patients who actually have cancer, in comparison to 375 who do not actually have cancer. The model performs pretty well with an accuracy of 0.95, or is it? Not really, the model is pretty good at predicting the patients who do not have cancer, but we can not say the same for the patients who actually have cancer (only predicted 10 out of 25 who actually have cancer correctly). Because the accuracy only considers the ratio of correct predictions to total predictions, the most frequent class performance dominates the metric, leading to misleading interpretation of model performance when we have an imbalanced dataset. . To calculate accuracy we will be using the accuracy_score function from the sklearn library. . Documentation for accuracy_score() . y_pred = clf.predict(X_test) #calling the accuracy_score function from metrics accuracy_score = metrics.accuracy_score(y_test, y_pred) print(&quot;Accuracy score is: {:.2f}&quot;.format(accuracy_score)) . Accuracy score is: 0.96 . correct_predictions = [pred for i,pred in enumerate(y_pred) if pred == y_test[i]] accuracy_manual = len(correct_predictions)/len(y_pred) print(&quot;Accuracy score is: {:.2f}&quot;.format(accuracy_manual)) . Accuracy score is: 0.96 . Precision . Precision is the measure of the ratio of true positive predictions to the total positive predictions of the model. Precision is a useful metric in cases where false positives are more significant than false negatives. SUch cases can be recommendation systems where a false positives can lead to customer churn which in turn reduces business success. . $$ text{Precision} = frac{ text{TP}}{TP + FP} $$ . . The precision from our previous example would be: $$ text{Precision} = frac{10}{10 + 5} = 0.667 $$ . Not as good as the accuracy right? In this case false negatives are more important than false positives, we do not want to misdiagnose any cancer patient as negative if they actually have cancer! We also a have a metric for that, Recall! . To calculate accuracy we will be using the precision_score function from the sklearn library. . Documentation for precision_score() . precision_score = metrics.precision_score(y_test, y_pred) print(&quot;Precision score is: {:.2f}&quot;.format(precision_score)) . Precision score is: 0.95 . TP = [pred for i,pred in enumerate(y_pred) if (pred == y_test[i] &amp; pred == 1)] TP_FP = [pred for i,pred in enumerate(y_pred) if (pred == 1)] precision_manual = len(TP) / len(TP_FP) print(&quot;Precision score is: {:.2f}&quot;.format(precision_manual)) . Precision score is: 0.95 . Recall . Recall is the ratio of how many of the positive cases the model was able to correctly predict. Recall is a useful metric when false negatives are more significant than false positives. Like we mentioned, it is, for example, important in medical cases (like we have with cancer dataset) where we do not want to miss any actual positives whereas false alarms (false positives) can be considered less important. . $$ text{Recall} = frac{ text{TP}}{TP + FN} $$ . . The recall from our previous example would be: $$ text{Recall} = frac{10}{10 + 15} = 0.4 $$ . As we can see our recall is much morse compared to accuracy and precision, which, in this case is actually the most important metric. Here we can clearly see how using accuracy for imbalanced datasets can be very misleading. . To calculate accuracy we will be using the recall_score function from the sklearn library. . Documentation for recall_score() . recall_score = metrics.recall_score(y_test, y_pred) print(&quot;Recall score is: {:.2f}&quot;.format(recall_score)) . Recall score is: 0.99 . TP = [pred for i,pred in enumerate(y_pred) if (pred == y_test[i] &amp; pred == 1)] TP_FN = [pred for i,pred in enumerate(y_pred) if (y_test[i] == 1)] recall_manual = len(TP) / len(TP_FN) print(&quot;Precision score is: {:.2f}&quot;.format(recall_manual)) . Precision score is: 0.99 . F1-Score . F1- score is a combination of both precision and recall. It is the harmonic mean of precision and recall and is maximum when precision is equal to recall. So when we are looking for the best of both worlds (when we have no difference of significance between false positives and false negatives) f1 is our metric. . $$ text{F1-Score} = frac{2 * text{Precision}* text{Recall}}{Precision + Recall} $$ . The F1-score from our previous example would be: $$ text{F1-Score} = frac{2 *0.667*0.4}{0.667 + 0.40} = 0.5 $$ . F1-score penalizes the extreme values more and is an effective metric when adding more data with no effect on outcome or when we have a high number of true negatives. It is also a better option to use compared to accuracy when we are dealing with imbalanced data. . To calculate accuracy we will be using the f1_score function from the sklearn library. . Documentation for f1_score() . f1_score = metrics.f1_score(y_test, y_pred) print(&quot;F1-score is: {:.2f}&quot;.format(f1_score)) . F1-score is: 0.97 . f1_manual = 2*precision_manual*recall_manual/(precision_manual + recall_manual) print(&quot;F1-score is: {:.2f}&quot;.format(f1_manual)) . F1-score is: 0.97 . Precision-Recall Curve ( PR Curve) . PR curve is a graph illustrating the change of precision values with respect to the recall value for binary classification. PR curve is plotted by calculating the precision and recall values for different thresholds for a classifier and plotting the values. Ex. for logistic regression the threshold would be the predicted probability of a prediction belonging to the positive class. . It is often used in problems with imbalanced dataset. We have the precision values on the y-axis and the recall values on the x-axis. We saw in earlier examples for which use cases precision and recall metrics maybe suitable. Well, the PR curve allows us to consider both metrics and the change of one with respect to other. . Considering both precision and recall can be useful when we have an imbalance of samples between the two target classes. This is specially the case, when samples of negative class ( class 0) are more than then samples of positive class (class 1). Both precision and recall do not consider the number of true negatives and focus on the correct of false prediction of the positive class. . To plot the pr-curve for a classifier, we will be using the precision_recall_curve() function from the sklearn library. . Documentation for precision_recall_curve() . We can obtain the probabilities using the predict_proba() method, and then use these probabilities to construct the PR curve. The decision threshold is usually 0.5. This means if probability is larger than 0.5, then it will be predicted as positive. . from sklearn.metrics import precision_recall_curve import matplotlib.pyplot as plt %matplotlib inline probabilities = clf.predict_proba(X_test)[:,1] precision_vals,recall_vals,thresholds = precision_recall_curve(y_test, probabilities) plt.plot(recall_vals, precision_vals, label = &#39;PR Curve&#39;, marker=&#39;o&#39;) for i,val in enumerate(precision_vals): if precision_vals[i] != precision_vals[-1]: plt.text(recall_vals[i], val, str(round(thresholds[i],3)), fontsize=10) plt.axvline(x=1,ymin=0, ymax=0.95, color=&#39;r&#39;, label=&quot;Best Model&quot;) plt.axhline(y=1, xmin=0, xmax=0.95, color=&#39;r&#39;) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) plt.legend([&#39;PR Curve&#39;, &#39;Best model&#39;], loc = &#39;best&#39;) plt.show() . Here we have calculated the precision and recall values for each threshold and plotted them. The red lines represent what a perfect classifier would look like. Our PR curves also seems to be performing pretty well. On each marker we can also see the associated threshold probability value. We want to choose the threshold which lies closer the our red line of perfection. Here depending which metric we are aiming to maximimize, we can choose the threshold according to that. If we have no preference for recall or precision, We can also calculate the f1-score for each threshold and make a selection according to that. . f1_score = (2 * precision_vals * recall_vals) / (precision_vals + recall_vals) f1_score . array([0.96598639, 0.95890411, 0.96551724, 0.97222222, 0.97902098, 0.97183099, 0.9787234 , 0.98571429, 0.97841727, 0.97101449, 0.96350365, 0.95588235, 0.94814815, 0.94029851, 0.93233083, 0.92424242, 0.91603053, 0.90769231, 0.89922481, 0.890625 , 0.88188976, 0.87301587, 0.864 , 0.85483871, 0.84552846, 0.83606557, 0.82644628, 0.81666667, 0.80672269, 0.79661017, 0.78632479, 0.77586207, 0.76521739, 0.75438596, 0.74336283, 0.73214286, 0.72072072, 0.70909091, 0.69724771, 0.68518519, 0.6728972 , 0.66037736, 0.64761905, 0.63461538, 0.62135922, 0.60784314, 0.59405941, 0.58 , 0.56565657, 0.55102041, 0.53608247, 0.52083333, 0.50526316, 0.4893617 , 0.47311828, 0.45652174, 0.43956044, 0.42222222, 0.40449438, 0.38636364, 0.36781609, 0.34883721, 0.32941176, 0.30952381, 0.28915663, 0.26829268, 0.24691358, 0.225 , 0.20253165, 0.17948718, 0.15584416, 0.13157895, 0.10666667, 0.08108108, 0.05479452, 0.02777778, 0. ]) . import numpy as np ix = np.argmax(f1_score) print(&#39;Best Threshold=%f, F1-Score=%.3f&#39; % (thresholds[ix], f1_score[ix])) . Best Threshold=0.851244, F1-Score=0.986 . ROC Curve (Receiver Operating Characteristic Curve) . A ROC curve is a plot indicating the performance of a classification model for different classification thresholds by plotting True Positive Rate (Recall) against the False Positive Rate (FPR). It has a similar idea to PR Curve but we plot the FPR instead of precision in this curve. . $$ text{FPR} = frac{ text{FP}}{FP + TN} $$ . To plot the ROC-curve for a classifier, we will be using the roc_curve() function from the sklearn library. . Documentation for roc_curve() . from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_test, probabilities) plt.plot( fpr, tpr, label = &#39;ROC Curve&#39;, marker=&#39;o&#39;) for i,val in enumerate(tpr): plt.text(fpr[i], val, str(round(thresholds[i],3)), fontsize=10) plt.xlabel(&#39;FPR&#39;) plt.ylabel(&#39;TPR&#39;) plt.legend(loc = &#39;best&#39;) plt.show() . By lowering the classification threshold, more predictions are identified as positive. This increases both false positivi and true positive rates. Therefore, the curve looks different to that of a PR curve. . In ROC curve, we are looking for the ideal point which is closest to the top left corner. Having a number to estimate how close our curve is to the top left corner would be useful right? Area Under the Curve (AUC) is another method which can help us here. . Area Under the Curve (AUC) . AUC / Area Under the Curve is the entire two dimensional area under the ROC curve. AUC helps us summarise the information from a ROC curve (can also be used for PR curve) into a single value. . ROC curve is basically a curve of probabilities (threholds) and AUC basically is the measure of separability of the calsses. It tells us how well the model is able to predict the classes. Therefore, a higher AUC value means a better classifier. AUC value varies between 0 and 1, meaning a value of 1 would mean a perfect classifier. . To calculate the AUC score for roc, we will use the roc_auc_score() function from sklearn library. . Documentation for roc_auc_score() . Before continuing, let&#39;s plot our ROC-curve again to examine the area under the curve visually. . plt.plot( fpr, tpr, label = &#39;ROC curve&#39;) plt.fill_between(fpr, tpr, alpha=0.3) plt.xlabel(&#39;FPR&#39;) plt.ylabel(&#39;TPR&#39;) plt.legend(loc = &#39;best&#39;) plt.grid() plt.show() . A classifier model which can not discriminate between the two classes (a no-skill classifier) would predict the most frequent occuring class or a random class for all predictions and it would have an AUC of 0.5. Its ROC curve would be a diagonal line as below. A classifier which predicts wrong all the time (0% correct prediction) would look exactly opposite of the best curve. . from sklearn.metrics import roc_curve pnt_1 = [0, 1] pnt_2 = [0, 1] fpr, tpr, thresholds = roc_curve(y_test, probabilities) plt.plot( fpr, tpr, label = &#39;ROC curve for DecisionTree&#39;) plt.plot( pnt_1, pnt_2, label = &#39;ROC curve for AUC=0.5&#39;, color=&#39;r&#39;) plt.fill_between( pnt_1, pnt_2, alpha=0.3, facecolor=&#39;r&#39;) plt.axvline(x=1,ymin=0.05, ymax=0.95, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&quot;ROC curve for AUC=0.0&quot;) plt.axhline(y=0, xmin=0.05, xmax=0.95, color=&#39;r&#39;, linestyle=&#39;--&#39;) plt.xlabel(&#39;FPR&#39;) plt.ylabel(&#39;TPR&#39;) plt.legend(loc = &#39;best&#39;) plt.show() . We can use the roc_auc_score function from the sklearn library to calculate the ROC AUC score for the plot above . from sklearn.metrics import roc_auc_score roc_auc_score(y_test, probabilities) . 0.9977071732721913 . Advantages: . AUC is invariant to scale. It is a measure of how good the predictions are ranked, rather than their absolute values. | AUC is invariant to classification-threshold. It is a measure of the model&#39;s performance (correctness of predictions) irrespective of the classification threshold chosen. | . Disadvantages: . Invariance to scale is not always wanted. Sometimes we need the well adapted oribability outputs. | Invariance to threshold is not always wanted as well. When there are big differences in terms of false negatives and false positives, we may want to aim to minimize only one of these. For example, in out cancer example previously, we mentioned how important it is to minimize the false negatives. In this case, AUC will not be a usefull metric. | . PR Curve vs ROC Curve . ROC curves are suitable when we are dealing with more of a balanced dataset, whereas PR curves are suitable for imbalanced datasets. . | The PR cuve does not consider the true negatives, therefore it is a useful metric when we have an imbalanced dataset, if the negative class is the majority class, considering making correct negative predictions are not that critical for the problem. . | Since ROC curve also takes the true negatives into consideration, it is a better metric to use if the when also the negative class is important (when both TP and TN are important). | .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/09/14/Classification-Evaluation.html",
            "relUrl": "/jupyter/2022/09/14/Classification-Evaluation.html",
            "date": " • Sep 14, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Linear Regression in Machine Learning",
            "content": "Content prepared by: Berk Hakbilen . What is Regression? . In statistical modeling, regression analysis estimates the relationship between one or more independent variables and the dependent variable which represents the outcome. . To explain with an example you can imagine a list of houses, with information regarding to the size, distance to city center, garden (independent variables). Using these information, you can try to understand how the price(dependent variables) changes. . So for a regression analysis we have a set of observations or samples with one or more variables/features. Then, we define a dependent variable (the outcome) and try to find a relation between the dependent variables and the independent variables. The best way to do this is by finding a function which best represent the data. . Linear Models for Regression . Linear Regression . In the linear regression model, we will use regression analysis to best represent the dataset through a linear function. Then, we will use this function to predict the outcome of a new sample/observation which was not in the dataset. . Linear regression is one of the most used regression models due to its simplicity and ease of understanding the results. Let&#39;s move on to the model formulations to understand this better. . . The linear regression function is written assuming a linear relationship between the variables: . $$y = w_1x_1 + ... + w_nx_n + c$$ . where w terms are the regression coefficients, x terms are the independent variables or features, y is dependent variable/outcome and c is the constant bias term. . We can write a simple linear regression function for the houses examples we mentioned above. . $$y_{price} = 500X_{size} - 350X_{distance to city} + 400.000$$ So if we plug in the features of a new house into this function, we can predict its price (let&#39;s assume size is 150m2 and distance to city center is 5 km). $$y_{price} = 500*150 - 350*5 + 400.000 = 75.000 - 1750 + 400.000 = 476.750$$ . See how the coefficient of distance to city center is minus. Meaning closer to center, more expensive the house will be. . We can create a simple fake regression dataset with only one feature and plot it to see the data behaviour more clearly. . import matplotlib.pyplot as plt from sklearn.datasets import make_regression plt.figure() plt.title(&#39;Samples in a dataset with only one feature (dependent variable)&#39;) X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 40, random_state=42) plt.scatter(X, y, marker= &#39;o&#39;, s=50) plt.show() . The dataset above has only one dependent variable. In this case, the regression function would be: $$y = w_1x_1 + c$$ . where w1 would be the slope the curve and c would be the offset value. . When we train our model on this data, the coefficients and the bias term will be determined automatically so that the regression function best fits the dataset. . The model algorithm finds the best coefficients for the dataset by optimizing an objective function, which in this case would be the loss function. The loss function represents the difference between the predicted outcome values and the real outcome values. . Least-Squared Linear Regression . In the Least-Squared linear regression model the coefficients and bias are determined by minimizing the sum of squared differences (SSR) for all of the samples in the data. This model is also called Ordinary Least-Squares. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2}$$ . If we interpret the function, it is a function determined by taking the square of the difference between the predicted outcome value and the real outcome value. . Let&#39;s train the Linear Regression model using the fake dataset we previously created and have a look at the calculated coefficients. . from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42) model = LinearRegression() model.fit(X_train, y_train) print(&#39;feature coefficient (w_1): {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_train, y_train))) print(&#39;R-squared score (test): {:.3f}&#39; .format(model.score(X_test, y_test))) . feature coefficient (w_1): [101.41780398] intercept (c): 53.342 R-squared score (training): 0.891 R-squared score (test): 0.735 . Here, R^2 is the coefficient of determination. This term represents the amount of variation in outcome(y) explained by the dependence on features (x variables). Therefore, a larger R^2 indicates a better model performance or a better fit. . When R^2 is equal to one, then RSS is equals to 0. Meaning the predicted outcome values and the real outcome values are exactly the same. We will be using the R^2 term to measure the performance of our model. . plt.figure(figsize=(6,5)) plt.scatter(X, y, marker= &#39;o&#39;, s=50, alpha=0.7) plt.plot(X, model.coef_*X + model.intercept_, &#39;r-&#39;) plt.title(&#39;Least-squares linear regression model&#39;) plt.xlabel(&#39;Variable/feature value (x)&#39;) plt.ylabel(&#39;Outcome (y)&#39;) plt.show() . Ridge Regression - L2 Regularization . Ridge regression model calculates coefficients and the bias (w and c) using the same criteria in Least-Squared however with an extra term. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2} + boxed { alpha sum limits_{j=1}^{m} {w_j^2}}$$ . This term is a penalty to adjust the large variations in the coefficients. The linear prediction formula is still the same but only the way coefficients are calculated differs due to this extra penalty term. This is called regularization. It serves to prevent overfitting by restricting the variation of the coefficients which results in a less complex or simpler model. . This extra term is basically the sum of squares of the coefficients. Therefore, when we try to minimize the RSS function, we also minimize the the sum of squares of the coefficients which is called L2 regularization. Moreover, the alpha constant serves to control the influence of this regularization. This way, in comparison to the Least-Squared model, we can actually control the complexity of our model with the help of alpha term. The higher alpha term, higher the regularization is, and simpler the model will be. . The accuracy improvement with datasets including one dependent variable (feature) is not significant. However, for datasets with multiple features, regularization can be very effective to reduce model complexity, therefore overfitting and increase model performance on test set. . Let&#39;s have a look at its implementation in python. . from sklearn import datasets X,y = datasets.load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42) from sklearn.linear_model import Ridge model = Ridge() model.fit(X_train, y_train) print(&#39;feature coefficients: {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) . feature coefficients: [ 50.55155508 -67.72236516 278.3007281 197.62401363 -6.2459735 -26.22698465 -151.39451804 120.32319558 215.85469359 101.75554294] intercept (c): 152.514 . plt.figure(figsize=(10,5)) alphas = [1,5,10,20,50,100,500] features = [&#39;w_&#39;+str(i+1) for i,_ in enumerate(model.coef_)] for alpha in alphas: model = Ridge(alpha=alpha).fit(X_train,y_train) plt.scatter(features,model.coef_, alpha=0.7,label=(&#39;alpha=&#39;+str(alpha))) plt.axhline(0) plt.xticks(features) plt.legend(loc=&#39;upper left&#39;) plt.show() . Normalization can be applied unfairly to the features when they have different scales (when one feature has values around 0-1 and the other has from 100-1000). This can cause inaccuracies in our model when we apply regularization. In this case, feature scaling comes to our help to normalize all the values in the dataset, so that we can get rid of the scale differences. We will look in to feature scaling in another section... . Lasso Regression - L1 Regularization . Lasso regression is also a regularized linear regression model. In comparison to Ridge regression, it uses L1 regularization as the penalty term while calculating the coefficients. . Let&#39;s have a look at how the RSS function looks like with the penalty term for L1 regularization. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2} + boxed { alpha sum limits_{j=1}^{m} {|w_j|}}$$ . The penalty term for L1 regularization is the sum of absolute values of the coefficients. Therefore, when the algorithm tries to minimize RSS, it enforces the regularization by minimizing the sum of absolute values of the coefficients. . This results in coefficients of the least effective paramaters to be 0 which is kind of like feature selection. Therefore, it is most effectively used for datasets where there a few features with a more dominant effect compared to others. This results in eliminating features which have a small effect by setting their coefficients to 0. . Alpha term is again used to control the amount of regularization. . from sklearn.linear_model import Lasso model = Lasso() model.fit(X_train, y_train) print(&#39;feature coefficients: {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) . feature coefficients: [ 0. -0. 398.38436775 46.17884277 0. 0. -0. 0. 238.18740159 0. ] intercept (c): 152.944 . After finding the coefficients of the dominant features, we can go ahead and list their labels. . import numpy as np data = datasets.load_diabetes() np.take(data.feature_names,np.nonzero(model.coef_)) . array([[&#39;bmi&#39;, &#39;bp&#39;, &#39;s5&#39;]], dtype=&#39;&lt;U3&#39;) . alphas = [0.1,0.5,1,2,5,10] for alpha in alphas: model = Lasso(alpha=alpha).fit(X_train,y_train) print(&#39;feature coefficients for alpha={}: n{}&#39;.format(alpha,model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_train, y_train))) print(&#39;R-squared score (test): {:.3f} n&#39; .format(model.score(X_test, y_test))) . feature coefficients for alpha=0.1: [ 0. -151.41550421 540.85933829 337.22472854 -85.19030645 -0. -262.90345036 0. 418.24822392 9.92430726] R-squared score (training): 0.508 R-squared score (test): 0.502 feature coefficients for alpha=0.5: [ 0. -0. 492.28522027 187.79307581 -0. -0. -107.63391392 0. 322.2269439 0. ] R-squared score (training): 0.451 R-squared score (test): 0.477 feature coefficients for alpha=1: [ 0. -0. 398.38436775 46.17884277 0. 0. -0. 0. 238.18740159 0. ] R-squared score (training): 0.347 R-squared score (test): 0.379 feature coefficients for alpha=2: [ 0. 0. 75.24915126 0. 0. 0. -0. 0. 0. 0. ] R-squared score (training): 0.052 R-squared score (test): 0.041 feature coefficients for alpha=5: [ 0. 0. 0. 0. 0. 0. -0. 0. 0. 0.] R-squared score (training): 0.000 R-squared score (test): -0.014 feature coefficients for alpha=10: [ 0. 0. 0. 0. 0. 0. -0. 0. 0. 0.] R-squared score (training): 0.000 R-squared score (test): -0.014 . Ridge or Lasso? . To sum up, it makes sense to use the Ridge regression model there are many small to medium effective features. If there are only a few dominantly effective features, use the Lasso regression model. . Polynomial Regression . Linear regression performs well on the assumption that the relationship between the independent variables (features) and the dependent variable(outcome) is linear. If the distrubtion of the data is more complex and does not show a linear behaviour, can we still use linear models to represent such datasets? This is where polynomial regression comes in very useful. . To capture this complex behaviour, we can add higher order terms to represent the features in the data. Transforming the linear model with one feature: . $$y = w_1x + c rightarrow boxed{y=w_1x + w_2x^2 + c} $$ . Since the coefficients are related to features linearly, this is still a liner model. However, it contains quadratic terms and the curve fitted is a polynomial curve. . Let&#39;s continue with an example for Polynomial regression. To convert the features to higher order terms, we can use the PolynomialFeatures class from scikit-learn. Then we can use the Linear regression model from before to train the model. . But before, let us create a dataset which could be a good fit for a 2nd degree function. For that we will use numpy to create random X points and plug them into a representative function. . np.random.seed(0) X = 2 - 3 * np.random.normal(0, 1, 100) y = X - 2 * (X ** 2) + np.random.normal(-3, 3, 100) plt.scatter(X, y, s=10) plt.show() . We can reshape the arrays we created so that we can feed them in to the model. First, we will train a LinearRegression model to see how it fits to this data. . X = X[:, np.newaxis] y = y[:, np.newaxis] model = LinearRegression() model.fit(X,y) print(&#39;feature coefficients: n{}&#39;.format(model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X, y))) plt.plot(X, model.coef_*X + model.intercept_, &#39;r-&#39;) plt.scatter(X,y, s=10) plt.show() . feature coefficients: [[-6.36572661]] R-squared score (training): 0.397 . As expected, Linear Regression model does not provide a very good fit with the normal features for a dataset of this behaviour. Now, we can create 2nd order Polynomial features using the PolynomialFeatures class from sk-learn library. Then, use these new 2nd order features to train the same linear regression model. . from sklearn.preprocessing import PolynomialFeatures poly_features= PolynomialFeatures(degree=2) X_poly = poly_features.fit_transform(X) model = LinearRegression() model.fit(X_poly,y) print(&#39;feature coefficients: n{}&#39;.format(model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_poly, y))) plt.scatter(X,model.predict(X_poly),s=10,label=&quot;polynomial prediction&quot;) plt.scatter(X,y,s=10,label=&quot;real data&quot;) plt.legend(loc=&#39;lower left&#39;) plt.show() . feature coefficients: [[ 0. 0.96597113 -2.02225052]] R-squared score (training): 0.990 . This time, we were able to obtain a very good fit using the same linear regression model but with 2nd order features obtained from the PolynomialFeatures class. This is a perfect example to show how Polynomial Linear Regression can be used to obtain better fits with data which do not have a linear relationship between the features and the outcome value. .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/03/19/Linear-Regression.ipynb.html",
            "relUrl": "/jupyter/2022/03/19/Linear-Regression.ipynb.html",
            "date": " • Mar 19, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hakbilenberk.github.io/LearningML/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey there👋 My name is Berk Hakbilen. I am currently working as a data scientist. I transitioned into a career into data science after discovering my passion for data and developing myself in the field of data science as a hobby. During this transition, free online material available was a huge help for me. That’s why I decided to create this blog, to publish some of the free learning material that I have created in my free time, so that it can be helpful for others who are interested about data science! . I hope this material is useful for you. If you have any questions or feedback, feel free to contact me! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hakbilenberk.github.io/LearningML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hakbilenberk.github.io/LearningML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}