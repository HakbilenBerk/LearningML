{
  
    
        "post0": {
            "title": "Logistic Regression",
            "content": "Content prepared by: Berk Hakbilen . Logistic Regression Theory . Logistic Regression for Classification . Despite its name, logistic regression is a essential technique for binary classification and multiclass classification rather than a regression algorithm. It falls under the category of linear classifiers. It is a fast, and simple model, making it easier to interpret the results. . Logistic regression is like its name suggests also a regression analysis. However, unlike linear regression (which is not suitable for a classification analysis), the calculated result is the probability of an event. . Let&#39;s have a look at the linear regression model and then derive the logistic regression function from this. . Linear regression formula: $$y^i = a_0 + a_1x_1^i + .... + a_nx_n^i $$ . On the other hand the so called sigmoid function is: $$P(x) = frac{1}{1 + exp(-x)} $$ . and its curve: . Here we can see that its output is always a value ranging from 0 to 1 which is the exact behaviour we want for a binary classification. . Like we just mentioned, for a classification problem, we want to get probabilities between 0 - 1 as results. We can achieve that by substituting our linear regression function into our sigmoid function, we obtain our logistic regression function: $$P(y^i) = frac{1}{1 + exp(-(a_0 + a_1x_1^i + .... + a_nx_n^i))} $$ . Looking at the formula we can see that the regression coefficients are now superscript of the exponential term (e). This way the regression coefficients from the linear regression function still effect the probability outcome of the logistic function. . Exploratory Data Analysis . from sklearn.datasets import load_breast_cancer import pandas as pd cancer = load_breast_cancer() df = pd.DataFrame(cancer.data, columns=cancer.feature_names) df[&#39;result&#39;] = pd.Series(cancer.target) . df.head() . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension radius error texture error perimeter error area error smoothness error compactness error concavity error concave points error symmetry error fractal dimension error worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension result . 0 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | 1.0950 | 0.9053 | 8.589 | 153.40 | 0.006399 | 0.04904 | 0.05373 | 0.01587 | 0.03003 | 0.006193 | 25.38 | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | 0 | . 1 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | 0.5435 | 0.7339 | 3.398 | 74.08 | 0.005225 | 0.01308 | 0.01860 | 0.01340 | 0.01389 | 0.003532 | 24.99 | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | 0 | . 2 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | 0.7456 | 0.7869 | 4.585 | 94.03 | 0.006150 | 0.04006 | 0.03832 | 0.02058 | 0.02250 | 0.004571 | 23.57 | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | 0 | . 3 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | 0.2597 | 0.09744 | 0.4956 | 1.1560 | 3.445 | 27.23 | 0.009110 | 0.07458 | 0.05661 | 0.01867 | 0.05963 | 0.009208 | 14.91 | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | 0 | . 4 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | 0.1809 | 0.05883 | 0.7572 | 0.7813 | 5.438 | 94.44 | 0.011490 | 0.02461 | 0.05688 | 0.01885 | 0.01756 | 0.005115 | 22.54 | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 mean radius 569 non-null float64 1 mean texture 569 non-null float64 2 mean perimeter 569 non-null float64 3 mean area 569 non-null float64 4 mean smoothness 569 non-null float64 5 mean compactness 569 non-null float64 6 mean concavity 569 non-null float64 7 mean concave points 569 non-null float64 8 mean symmetry 569 non-null float64 9 mean fractal dimension 569 non-null float64 10 radius error 569 non-null float64 11 texture error 569 non-null float64 12 perimeter error 569 non-null float64 13 area error 569 non-null float64 14 smoothness error 569 non-null float64 15 compactness error 569 non-null float64 16 concavity error 569 non-null float64 17 concave points error 569 non-null float64 18 symmetry error 569 non-null float64 19 fractal dimension error 569 non-null float64 20 worst radius 569 non-null float64 21 worst texture 569 non-null float64 22 worst perimeter 569 non-null float64 23 worst area 569 non-null float64 24 worst smoothness 569 non-null float64 25 worst compactness 569 non-null float64 26 worst concavity 569 non-null float64 27 worst concave points 569 non-null float64 28 worst symmetry 569 non-null float64 29 worst fractal dimension 569 non-null float64 30 result 569 non-null int64 dtypes: float64(30), int64(1) memory usage: 137.9 KB . import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) sns.countplot(df[&#39;result&#39;]) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd242d02950&gt; . cancer.target_names . array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;) . benign, malignant = df[&#39;result&#39;].value_counts() print(&#39;Number of benign results {} corresponding to {} percent: &#39;.format(benign,round(benign / len(df) * 100, 2))) print(&#39;Number of malignant results {} corresponding to {} percent: &#39;.format(malignant,round(malignant / len(df) * 100, 2))) . Number of benign results 357 corresponding to 62.74 percent: Number of malignant results 212 corresponding to 37.26 percent: . cols = [&#39;result&#39;, &#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;] sns.pairplot(data=df[cols], hue=&#39;result&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x7fd2423c4990&gt; . f, ax = plt.subplots(figsize=(20, 20)) corr = df.corr().round(2) # Create a mask for the lower triangle import numpy as np mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # create the heatmap sns.heatmap(corr, mask=mask, square=True, annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd2348f62d0&gt; . Logistic Regression Model . from sklearn.model_selection import train_test_split y = df[&#39;result&#39;].values X = df.drop(&#39;result&#39;,axis=1).values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . X_train.shape . (455, 30) . from sklearn.linear_model import LogisticRegression . model = LogisticRegression(max_iter=10000) . logisticregression = model.fit(X_train,y_train) . print(&quot;training set score: %f&quot; % logisticregression.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression.score(X_test, y_test)) . training set score: 0.960440 test set score: 0.956140 . Because our training and test score are closer to each other, we can see that we are actually underfitting. Let&#39;s try a higher C value which means a more complex model which fits better to the data. . model = LogisticRegression(max_iter=10000,C=1000) logisticregression_1000 = model.fit(X_train,y_train) print(&quot;training set score: %f&quot; % logisticregression_1000.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression_1000.score(X_test, y_test)) . training set score: 0.986813 test set score: 0.991228 . Because our training and test score are closer to each other, we can see that we are actually underfitting. Let&#39;s try a higher C value which means a more complex model which fits better to the data. . model = LogisticRegression(max_iter=10000,C=0.1) logisticregression_0_1 = model.fit(X_train,y_train) print(&quot;training set score: %f&quot; % logisticregression_0_1.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression_0_1.score(X_test, y_test)) . training set score: 0.949451 test set score: 0.964912 . A lower C does not create a big difference since our model is already underfitting to the data. . from sklearn.metrics import confusion_matrix y_pred = model.predict(X_test) cf_matrix = confusion_matrix(y_test, y_pred,labels=[0,1]) cf_matrix . array([[40, 3], [ 1, 70]]) . sns.heatmap(cf_matrix, annot=True, cmap=&#39;Blues&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f895589abd0&gt; . from sklearn.metrics import plot_confusion_matrix disp = plot_confusion_matrix(model, X_test, y_test, display_labels=[&#39;Benign&#39;,&#39;Malignant&#39;]) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator. warnings.warn(msg, category=FutureWarning) . from sklearn import metrics print(metrics.classification_report(y_test, y_pred, target_names=[&#39;Benign&#39;,&#39;Malignant&#39;])) . precision recall f1-score support Benign 0.98 0.93 0.95 43 Malignant 0.96 0.99 0.97 71 accuracy 0.96 114 macro avg 0.97 0.96 0.96 114 weighted avg 0.97 0.96 0.96 114 . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9649122807017544 Precision: 0.958904109589041 Recall: 0.9859154929577465 .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/11/09/Logistic-Regression.html",
            "relUrl": "/jupyter/2022/11/09/Logistic-Regression.html",
            "date": " • Nov 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Learning Curves in Machine Learning",
            "content": "import numpy as np np.random.seed(0) X = 2 - 3 * np.random.normal(0, 1, 30) y = X - 2 * (X ** 2) + np.random.normal(-10, 10, 30) pnt_1 = [-5, 10] pnt_2 = [0,-185] plt.title(&quot;High Bias (Underfitting)&quot;) plt.scatter(X,y, s=10, label=&#39;training data points&#39;) plt.plot(pnt_1,pnt_2, color=&#39;r&#39;, label=&#39;model fit&#39;) plt.legend(loc=&#39;best&#39;) plt.grid() plt.show() . Variance is the amount of variability of the predictions made by the model. It can also be defined as the amount of which the predictions change as we change the training dataset. Model with high variance captures too much of the details in the training dataset resulting in a complex model failing to generalize on test data. A complex model which is overfitting to the training data yields very a very low error rate on the training set but a high error rate on the test set. . Xs, ys = zip(*sorted(zip(X, y))) plt.title(&quot;High Variance (Overfitting)&quot;) plt.scatter(Xs,ys, s=10, label=&#39;training data points&#39;) plt.plot(Xs, ys, color=&#39;r&#39;, label=&#39;model fit&#39;) plt.legend(loc=&#39;best&#39;) plt.grid() plt.show() . plt.scatter(X,y, label=&quot;training data points&quot;) p = np.polyfit(X,y,2) xfit=np.linspace(min(X),max(X), 1000) yfit=np.polyval(p, xfit) plt.plot(xfit,yfit,color=&#39;r&#39;,label=&#39;model fit&#39;) plt.grid() plt.title(&quot;Low Bias and Low Variance - Ideal Fit&quot;) plt.legend(loc=&#39;best&#39;) plt.show() . If we want to keep the error of our model low, then we should try to reduce bias and variance in the model. However, it is usually not possible to have both low bias and low variance. Therefore, a trade-off is necessary and this is where the learning curves come in handy . Remembering from section MSE: . $$ Err(x) = Var(x) + (Bias(x))^2 $$ . . Having talked about bias and variance and the effect of model complexity on these terms. We can continue with learning curves. Learning curve tells us how the error changes as the training set size increases. . Imagine that we first have a single data point in our training set. If we fit a model to that one point, we will be achieving 0 as error because it is very easy to pefectly fit the model on a single point. However, the same model will be performing very bad on a validation set of a normal size because it won&#39;t learn enough to actually capture the behaviour in the data (ofcourse we do not have a behaviour with one data point). As we increase the number of data points in the training set, it will be more difficult to perfectly fit the model to all the data points, hence trying to generalize to them minimizing error on all points. This will introduce an error on the training data set which will increase as the training set size increases. But now the model&#39;s error on validation set starts to decrease because the model is able to capture more of the data behaviour as the training size increases. . This is exacltly what we use the learning curves for: to observe the error change on training and validation sets as the training set size increases. . . We can use the diabetes dataset from sklearn to create some learning curves. Moreover, we can use the learning_curve() functions from the scikit-learn library to create the learning curves. As the model, let&#39;s use the linear regression model... . from sklearn import datasets from sklearn.linear_model import LinearRegression from sklearn.model_selection import learning_curve X,y = datasets.load_diabetes(return_X_y=True) X.shape . (442, 10) . We can choose training data size range of the way upto a value of 350, which is around %80 percent of the complete data we have. (80/20 rule for train(test split). . train_sizes = [1, 25, 50, 100, 200, 350] train_sizes, train_scores, val_scores = learning_curve( estimator = LinearRegression(), X = X, y = y, train_sizes = train_sizes, cv = 5, scoring = &#39;neg_root_mean_squared_error&#39;) . import pandas as pd print(&#39;Training scores: n&#39;, pd.DataFrame(train_scores, columns=[i+1 for i in range(5)] , index=train_sizes)) print(&#39;-&#39; * 100) print(&#39;Validation scores: n&#39;, pd.DataFrame(val_scores, columns=[i+1 for i in range(5)], index=train_sizes)) . Training scores: 1 2 3 4 5 1 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000 25 -33.788324 -26.697406 -26.697406 -26.697406 -26.697406 50 -47.394390 -43.688204 -43.688204 -43.688204 -43.688204 100 -51.148037 -47.923435 -49.509014 -49.509014 -49.509014 200 -54.262984 -53.293178 -52.896831 -51.943824 -51.943824 350 -54.091093 -53.466411 -53.064418 -53.721462 -53.600397 - Validation scores: 1 2 3 4 5 1 -73.703018 -80.514358 -79.111903 -72.716136 -80.552919 25 -115.490364 -103.656024 -94.553585 -89.170892 -98.510707 50 -66.670137 -61.905990 -61.447323 -60.288666 -58.484828 100 -56.814088 -60.964196 -58.414453 -58.260307 -58.087711 200 -53.981901 -55.786144 -56.516296 -55.676070 -54.967804 350 -52.750311 -55.047255 -56.904160 -54.898415 -54.042706 . We see 5 scores for each training size because of cross validations with 5 folds. We can average all the folds to obtain only one average metric for each training size. Also, because we used negative RMSE earlier, we can flip the signs back to psitive by multiplying with (-). . import pandas as pd mean_train_scores = -train_scores.mean(axis = 1) mean_val_scores= -val_scores.mean(axis = 1 ) print(&#39;Mean training scores n n&#39;, pd.Series(mean_train_scores, index = train_sizes)) print(&#39; n&#39;, &#39;-&#39; * 20) # separator print(&#39; nMean validation scores n n&#39;,pd.Series(mean_val_scores, index = train_sizes)) . Mean training scores 1 -0.000000 25 28.115590 50 44.429441 100 49.519703 200 52.868128 350 53.588756 dtype: float64 -- Mean validation scores 1 77.319667 25 100.276314 50 61.759389 100 58.508151 200 55.385643 350 54.728569 dtype: float64 . import matplotlib.pyplot as plt plt.plot(train_sizes, mean_train_scores, label = &#39;Training error&#39;) plt.plot(train_sizes, mean_val_scores, label = &#39;Validation error&#39;) plt.ylabel(&#39;RMSE&#39;) plt.xlabel(&#39;Training set size&#39;) plt.title(&#39;Learning curves&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fc214a18c90&gt; . Here on the graph, we can observe the behaviour we just mentioned (except for the point where training size is 1. This one point seems to yield a low MSE on validation, which is totally by coincidence. The model fit on one point by coindence seems to perform better than at 25). The training error at training size 1, is equal to 0 as we expected. . We can see that the linear regression model does not perform perfectly because we have a training RMSE of around 55 where both validation and training error converges. We can see that from training size of 200, both the training and validation RMSE does not change much. Therefore, if we want a better model performance, adding more training points is not going to help us here. Instead of collecting more training data, we should try a more complex ML model/algorithm. Moreover, adding more relevant features to data can also help since it will increase model complexity as well. . . . Well, how can we see if we have a high bias or variance? . A sign for a high bias in the model is high validation error. Our validation error converges to around 55, which is pretty high looking at the mean of the target values (around 150). So we can say that we have a bias problem. . But do we have high bias or low bias in the model? Low bias means, that the model fits very well to training data (overfitting) resulting in a low training error. High bias means that the model is too simple to capture important behaviour (does not fit well to training data) in the training data, meaning high training error. Since our training curve also converges to the value of validation curve which we considered to be high, we can say that we have a high bias problem. . . . To see if we have a variance problem we can check two things: . Observing the gap between the validation and training curve | By watching the training error as the training set increases | . High variance means that the model is fitting too well to the training data, which will result in a large validation error because it will fail to generalize. However, fitting too well to the training data will also yield a low training error. This means that we expect to have a large gap between the validation and training curves as they flatten. For low variance, the opposite of this holds. Hence, lower gaps means lower variance. . Moreover, High training error alone is also a good way to detect low variance. Low variance means, our algorithm is too simple and underfits the training data, which will result in a high training error. Hence high training alone indicates low variance problem in the model. . . . In our case, we can confirm that we have a small gap, hence a large training error which means a low variance in the model. . Finally we can confirm that our model has: . High bias and low variance, underfitting the training data. | Adding more training data points is not likely to help since both validation and training curves have converged/flattened. | . At this point, the next step would be to select a complexer algorithm. Adding more features can also be an option. However, since we can not add additional features to the dataset, we would have to generate new features using feature engineering technique which is not the scope here. Moreover, if the model has regularization, decrease it would also help. We use regularization to prevent overfitting hence if we decrease the regularization, the model has a better fit on training decreasing bias and increasing variance, which is also not really the scope here. . At this stage, a complexer model should help us decrease the bias and increase the variance. Let&#39;s try the RandomForestRegressor model from sklearn library: . from sklearn.ensemble import RandomForestRegressor train_sizes, train_scores, val_scores = learning_curve( estimator = RandomForestRegressor(), X = X, y = y, train_sizes = train_sizes, cv = 5, scoring = &#39;neg_root_mean_squared_error&#39;) mean_train_scores = -train_scores.mean(axis = 1) mean_val_scores= -val_scores.mean(axis = 1 ) plt.plot(train_sizes, mean_train_scores, label = &#39;Training error&#39;) plt.plot(train_sizes, mean_val_scores, label = &#39;Validation error&#39;) plt.ylabel(&#39;RMSE&#39;) plt.xlabel(&#39;Training set size&#39;) plt.title(&#39;Learning curves&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fc214a2fe10&gt; . Our training error seems to have converged to a much lower value than with linear regression model. Therefore, we can say that we have managed to decrease the bias amount. However, now we have a much larger gap between the training and validation curves, indicating a higher variance. Basically, our model fits pretty well to the training data but fails to generalize and perform well on the validation data indicating an overfitting problem. I think we are thinking of the same thing here ?! Bingo! we need to decrease the model complexity. . There a few ways to decrease model complexity with a RandomForest model and one of them is to adjust the maximum depth of each decision tree. We can do that by specifying the max_depth parameter when we define the RandomForestRegressor model. . from sklearn.ensemble import RandomForestRegressor train_sizes, train_scores, val_scores = learning_curve( estimator = RandomForestRegressor(max_depth=5), X = X, y = y, train_sizes = train_sizes, cv = 5, scoring = &#39;neg_root_mean_squared_error&#39;) mean_train_scores = -train_scores.mean(axis = 1) mean_val_scores= -val_scores.mean(axis = 1 ) plt.plot(train_sizes, mean_train_scores, label = &#39;Training error&#39;) plt.plot(train_sizes, mean_val_scores, label = &#39;Validation error&#39;) plt.ylabel(&#39;RMSE&#39;) plt.xlabel(&#39;Training set size&#39;) plt.title(&#39;Learning curves&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fc214a26dd0&gt; . We have managed to decrease the gap which means we were able to reduce model complexity hence reduce variance. However, the gap was reduced only by increasing the training error and with a slight improvement of validation error. We will not spend further time trying to improve model performance on this case since we have already cleared the most important concepts about learning curves. However as a last note, some steps to improve this model further would be: . Adding more features | Feature engineering or selection | Hyperparameter optimization to obtain the best hyperparameters for this model | . References . learning curves ML | .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/11/06/Learning-Curves-ML.html",
            "relUrl": "/jupyter/2022/11/06/Learning-Curves-ML.html",
            "date": " • Nov 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Evaluation for Regression",
            "content": "Evaluation for Regression . Model evaluation is very important since we need to understand how well our model is performing. In comparison to classification, performance of a regression model is slightly harder to determine because, unlike classification, it is almost impossible to predict the exact value of a target variable. Therefore, we need a way to calculate how close our prediction value is to the real value. . There are different model evaluation metrics that are used popularly for regression models which we are going to dive into in the following sections. . Mean Absolute Error . Mean absolute error is a very intuitive and simple technique, therefore also popular. It is basically the average of the distances between the predicted and the true values. Basically the distances between the predicted and the real values are also the error terms. The overall error for the whole data is the average of all prediction error terms. We take the absolute of the distances/errors to prevent negative and positive terms/errors from cancelling each other. . $$ MAE = frac{1}{N} sum limits_{i=1}^{N}{|y_i - ŷ_i|} $$ . Advantages . MAE is not sensitive to outliers. Use MAE when you do not want outliers to play a big role in error calculated. | . Disadvantages . MAE is not differentiable globally. This is not convenient when we use it as a loss function, due to the gradient optimization method. | . Mean Squared Error (MSE) . MSE is one of widely used metrics for regression problems. MSE is the the measure of average of squared distance between the actual values and the predicted values. Squared terms help to also take into consideration of negative terms and avoid cancellation of the total error between positive and negative differences. . $$ MSE = frac{1}{N} sum limits_{i=1}^{N}{(y_i - ŷ_i)^2} $$ . Advantages . Graph of MSE is differantiable which means it can be easily used as a loss function. | MSE can be decomposed into variance and bias squared. This helps us understand the effect of variance or bias in data to the overall error. | . $$ MSE(ŷ) = Var(ŷ) + (Bias(ŷ))^2 $$ . Disadvantages . The value calculated MSE has a different unit than the target variable since it is squared. (Ex. meter --&gt; meter^2) | If there exists outliers in the data, then they are going to result in a larger error. Therefore, MSE is not robust to outliers (this can also be an advantage if you are looking to penalize outliers). | . Root Mean Squared Error (RMSE) . As the name already suggests, in RMSE we take the root of the mean of squared distances, meaning the root of MSE. RMSE is also a popularly used evaluation metric, especially in deep learning techniques. . $$ RMSE = sqrt{ frac{1}{N} sum limits_{i=1}^{N}{(y_i - ŷ_i)^2} }$$ . Advantages . The error calculated has the same unit as the target variables making the interpretation relatively easier. | . Disadvantages . Just like MSE, RMSE is also susceptible to outliers. | . R-Squared . R square is a different metric compared to the ones we have discussed until now. It does not directly measure the error of the model. . R-squared evaluates the scatter of the data points around the fitted regression line. It is the percentage of the target variable variation which the model considers compared to the actual target variable variance. It is also known as the &quot;coefficient of determination&quot; or goodness of fit. . $$ R^2 = frac{ text{Variance considered by model}}{ text{Total variance}} $$ . $$ R^2 = 1 - frac{SS_{regression}}{SS_{total}} = 1 - frac{ sum{(y_i - ŷ_i)^2}}{ sum{(y_i - y_{mean})^2}} $$ . As we can see above, R-squared is calculated by dividing the sum of squared error of predictions by the total sum of square, where predicted value is replaced by the mean of real values. . R-squared is always between 0 and 1. 0 indicates that the model does not explain any of the variation in the target variable around its mean value. The regression model basically predicts the mean of the parget variable. A value of 1 indicates, that the model explains all the variance in the target variable around its mean. . A larger R-squared value usually indicates that the regression model fits the data better. However, a high R-square model does not necessarily mean a good model. . import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression import seaborn as sns; sns.set_theme(color_codes=True) X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 15, random_state=42) plt.figure() ax = sns.regplot(x=X,y=y) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) . R-squared score: 0.977 . X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 200, random_state=42) plt.figure() ax = sns.regplot(x=X,y=y) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) . R-squared score: 0.294 . Advantages . R-square is a handy, and an intuitive metric of how well the model fits the data. Therefore, it is a good metric for a baseline model evaluation. However, due to the disadvantages we are going to discuss now, it should be used carefully. | . Disadvantages . R-squared can&#39;t determine if the predictions are biased, that is why looking at the residual plots in addition is a good idea. . | R-squared does not necessarily indicate that a regression model is good to go. It is also possible to have a low R-squared score for a good regression model and a high R-squared model for a bad model (especially due to overfitting). . | When new input variables (predictors) are added to the model, the R-square is going to increase (because we are adding more variance to the data) independent of an actual performance increase in model. It never decreases when new input variables are added. Therefore, a model with many input variables may seem to have a better performance jsut because it has more input variables. This is an issue which we are going to address with adjusted R-squared. . | . It is still possible to fit a good model to a dataset with a lot of variance which is likely going to have a low R-square. However, it does not necessarily mean the model is bad if it is still able to capture the general trend in the dataset, and capture the effect of change of a predictor on the target variables. R-square becomes a big problem when we want to predict a target variable with a high precision, meaning with a small prediction interval. . A high R-squared score also does not necessarily mean a good model because it is not able to detect the bias. Therefore, also checking the residual plots is a good idea. Like we mentioned previously, a model with a high R-squared score can also be overfitting since it captures most of the variance in the data. Therefore, it is always a good idea to check the R-squared score of the predictions from the model and compare it to the R-squared score from the training data. . Adjusted R-Squared . We previously mentioned that R-square score never actually decreases but increases when we add more input variables because we increase the variance in the data. To address this issue, we are going to talk about adjusted R-squared. . The adjusted R-sqaured is adjusted version of R-square where the number of input variables in the model are also considered. R-square can penalize the additional input variables given they do not contribute to the model performance. Let&#39;s have a look at how it is calculated. . $$ R_a^2 = 1- (( frac{n-1}{n-k-1})(1-R^2)) $$ . where: . $n = text{number of data points}$, $k= text{number of input variables}$ . As number of input variables/features increase, the denominator will decrease, R-squared will increase slightly or remain constant if the added features are not relevant, meaning the complete term in paranthesis is going to increase. The resultant adjusted R-squared score is going to decrease because we deducted the increasing term from 1. . However, if the added input variables are relevant, then the R-squared score will increase much more and the term (1-R2) will decrease a lot. When we subtract the complete term in paranthesis from 1, the overall adjusted R-squared score will increase. You see how the number of added variables and the amount of R-squared increase, help us adjust the R-squared score to account for new variables depending on if they are relevant or not. . def get_adj_r2(X,y): return 1 - (1-model.score(X, y))*(len(X) - 1) / (len(X) - (X.shape[1] - 1) - 1) . X, y = make_regression(n_samples = 80, n_features=4, n_informative=1, bias = 50, noise = 60, random_state=42) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) print(&#39;Adjusted R-squared score: {:.3f}&#39; .format(get_adj_r2(X,y))) . R-squared score: 0.582 Adjusted R-squared score: 0.566 . X, y = make_regression(n_samples = 80, n_features=4, n_informative=1, bias = 50, noise = 60, random_state=42) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) print(&#39;Adjusted R-squared score: {:.3f}&#39; .format(get_adj_r2(X,y))) . R-squared score: 0.582 Adjusted R-squared score: 0.566 .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/11/06/Evaluation-Regression.ipynb.html",
            "relUrl": "/jupyter/2022/11/06/Evaluation-Regression.ipynb.html",
            "date": " • Nov 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Unsupervised Learning",
            "content": "Unsupervised Learning is the use of algortihms to extract and identify patterns and information from the data which is not labelled. Difference of unsupervised learning in comparison to supervised learning is that we do not tell the algorithm which input data results in which target/output but we provide the algorithm the data as a whole and allow the algorithm to identify patterns in the data on its own. . Clustering . Clustering is the most often used technique for unsupervised learning. Clustering groups the data points which are similar to each other in terms of the features of the data. . . There are many clustering types available; centroid model, connectivity model, distribution model, density model etc. We will be focusing on the mostly used three categories: . Centroid model: This type of clustering is an iterative process where the similarity of the data points are based on the distance to the centroid of the clusters in one or multidimensional space. One very popular model which falls under this category is the K-means clustering. . | Connectivity model: In this types of model the data points are considered more similar to nearby points in comparison than those farther away. These models can be either agglomerative or divisive. In the agglomerative approach, all data points are grouped into many clusters and then these clusters are merged into bigger ones with respect to the distances between them. In the divisive approach, all the data points are grouped into one cluster and then divided into smaller clusters with increasing distance. Hierarchical clustering is a well known and popular example for this model. This model does not handle large datasets very well. . | Density model: This type of models cluster the data points in one or multidimensional space based on determining areas of higher density (high number of data points in a small space) compared to the rest of the data. One often used example is DBSCAN. . | . Distance Measures . Until now we have discussed how clustering groups similar data points together. This is done by using different types of distance measures to determine if the data points are similar. Smaller distance between data points means more similar data points. Which distance measure to use depends very much on the type of data and problem that we are dealing with and it is an important factor. THe most used distance measures are: . Eucledian distance | Manhattan distance | . Where Eucledian distance is usually the default distance measure for most of the clustering models. We have already seen how these distance measures work in k Nearest Neighbors section. Therefore, we will not go into detail again. However, we can calculate the Eucledian distances for a simple example so that the concept is clear. Below is an example dataset containing three samples and their three features (F1, F2, F3): . F1 F2 F3 . Sample 1 | 0 | 1 | 1 | . Sample 2 | 2 | 4 | 0 | . Sample 3 | 1 | 0 | 2 | . Remembering the formula for Eucledian distance between two points, where x_i represents the feature of x for every sample i, and y_i represents the feature of y for every sample i: . $$ sqrt{ sum limits_{i=1}^{l}({x_i-y_i})^2} $$ . $$ d(sample_1, sample_2) = sqrt{ sum limits_{i=1}^{3}({ Delta_{sample_1,sample_2}F_i})^2} $$ . $$ d(sample_1, sample_2) = sqrt{(0-2)^2 + (1-4)^2 + (1-0)^2} = sqrt{14} $$ $$ d(sample_1, sample_3) = sqrt{(0-1)^2 + (1-0)^2 + (1-2)^2} = sqrt{3} $$ $$ d(sample_2, sample_3) = sqrt{(2-1)^2 + (4-0)^2 + (0-2)^2} = sqrt{21} $$ . The resulting distance matrix becomes: . Sample 1 Sample 2 Sample 3 . Sample 1 | $$0$$ | $$ sqrt{14}$$ | $$ sqrt{3}$$ | . Sample 2 | $$ sqrt{14}$$ | $$0$$ | $$ sqrt{21}$$ | . Sample 3 | $$ sqrt{3}$$ | $$ sqrt{21}$$ | $$0$$ | . Have you realised that diagonal is always zero and we ended up with a symmetric matrix? Well that is due to the way we calculate the distance with the Eucledian measure. The distance of a point to itself is always zero (diagonal) and the distance between sample 1 to sample 2 is same as sample 2 to sample 1 (symmetric) which makes very much sense... . In addition there are also correlation based distance measures such as Pearson correlation distance, Eisen cosine correlation distance, Spearman correlation distance and Kendall correlation distance. As the name already expresses, these measures consider two data points to be similar if their features are highly correlated. That means two data points having highly correlated features can be clustered together even if their eucledian distance is large. We will not go into detail for correlation based distances but keep in mind that Pearson&#39;s correlation is sensitive to outlier data. With datasets containing outliers a better option might be Spearman&#39;s correlation. . Connectivity model: Hierarchical Clustering . As the name suggests, hierarchical clustering creates a hierarchy between the clusters. It does this by first assigning a cluster to every data point (for a dataset with size n, we get n clusters) and then the two nearest(most similar) clusters are merged together to form a new larger cluster. This merging of clusters continues iteratively until there is one whole cluster left. This way, the algorithm builds a hierarchy of clusters, thus hierarchical clustering. . There are two main types of hierarchical clustering: . Agglomerative hierarchical clustering | Divisive hierarchical clustering | . Agglomerative Hierarchical Clustering . In Agglomerative hierarchical clustering the clustering starts with a cluster for every data point. Then, the nearest (most similar) pair of clusters are merged together. This merging process is done iteratively until there is only one big cluster left. . . Divisive Hierarchical Clustering . Divisive Hierarchical Clustering works the other way around. It starts with one cluster and assigns all the data points to that cluster. Afterwards, we split the farthest points in the cluster to seperate clusters. This splitting process is done iteratively until every cluster has one data point only. . . Let&#39;s perform hierarchical clustering on iris dataset from sklearn: . from sklearn.datasets import load_iris iris_data = load_iris() . Checking the features in our dataset, we can see that we have four features for each flower: sepal length, sepal width, petal length, petal width . iris_data.feature_names . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . Checking the target names, the flowers are classified into three different classes: setosa, versicolor, virginica. Don&#39;t get confused, we will not use the target names since we are dealing which clustering(unsupervised learning) here. This is just to give us a better understand of the dataset... . iris_data.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . data = iris_data.data . Dendrograms . Dendrograms are diagrams which are very helpful in visualising the hierarchical clustering structure. We can create a dendrogram to visualize the hierarchical clustering structure for the iris dataset: . from scipy.cluster.hierarchy import dendrogram, linkage, fcluster import matplotlib.pyplot as plt # generate the linkage matrix Z = linkage(data, method=&#39;complete&#39;) #method as &#39;complete&#39; is also known by the Farthest Point Algorithm plt.figure(figsize=(25, 10)) plt.title(&#39;Hierarchical Clustering Dendrogram for Iris Dataset&#39;) plt.xlabel(&#39;Species&#39;) plt.ylabel(&#39;Distance&#39;) dendrogram( Z, #truncate_mode=&#39;lastp&#39;, # show only the last p merged clusters #p=150, # leaf_rotation=90., # rotates the x axis labels leaf_font_size=8., # font size for the x axis labels ) max_d = 3.3 # max_d as in max_distance plt.axhline(y=max_d, c=&#39;k&#39;) plt.show() . Flatten the hierarchical cluster that we have obtained from linkage using the fcluster (flat cluster) with our number of cluster selection equals to three. We can assign the cluster as a new column to the dataframe. But before that we need to convert our data to a dataframe... . clusters = fcluster(Z,3, criterion=&#39;maxclust&#39;) print(clusters) . [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 2 1 2 1 2 1 2 2 2 2 1 2 1 2 2 1 2 1 2 1 1 1 1 1 1 1 2 2 2 2 1 2 1 1 1 2 2 2 1 2 2 2 2 2 1 2 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] . plt.scatter(data[clusters == 1, 0], data[clusters == 1, 1], label = &#39;Type 1&#39;) plt.scatter(data[clusters == 2, 0], data[clusters == 2, 1], label = &#39;Type 2&#39;) plt.scatter(data[clusters == 3, 0], data[clusters == 3, 1], label = &#39;Type 3&#39;) plt.title(&#39;Clusters&#39;) plt.show() . Now we know that three clusters would be a good decision for clustering the iris data. We can use the AgglomerativeClustering model with 3 clusters and compare the results to the target data we have. Since we use the same linkage method, we expect to get the same clustering results as the dendrogram . from sklearn.cluster import AgglomerativeClustering classifier = AgglomerativeClustering(n_clusters = 3, affinity = &#39;euclidean&#39;, linkage = &#39;complete&#39;) clusters = classifier.fit_predict(data) . Using the clusters we have obtained from AgglomerativeClustering model, we can plot the first two features as a scatter plot and compare it to the scatter plot of the target data from the dataset. . plt.scatter(data[clusters == 0, 0], data[clusters == 0, 1], label = &#39;Type 1&#39;) plt.scatter(data[clusters == 1, 0], data[clusters == 1, 1], label = &#39;Type 2&#39;) plt.scatter(data[clusters == 2, 0], data[clusters == 2, 1], label = &#39;Type 3&#39;) plt.title(&#39;Clusters&#39;) plt.show() . y=iris_data.target plt.scatter(data[y == 0, 0], data[y == 0, 1], label = &#39;Type 1&#39;) plt.scatter(data[y == 1, 0], data[y == 1, 1], label = &#39;Type 2&#39;) plt.scatter(data[y == 2, 0], data[y == 2, 1], label = &#39;Type 3&#39;) . &lt;matplotlib.collections.PathCollection at 0x7fc36fefaa10&gt; . We have obtained a very similar clustering in comparison to the classification in the data except for a few data points. This way we were able to group similar species together using their features. . Centroid Model: K-means Clustering . K-means clustering clustering is an iterative process where the similarity of the data points are based on the distance to the centroid of the clusters in one or multidimensional space. The algorithm partitions the data into K clusters where a data point in only in one cluster. The aim is to collect as similar as possible data points in one cluster while keeping the cluster as farther as possible. Algorithm does this by making sure that the sum of the squared distance between the datapoints and the cluster&#39;s centroid is minimum by minimizing the function known as squared error function: . $$ J(v) = sum limits_{i=1}^{k} sum limits_{j=1}^{c_i}{||x_i-v_j||^2} $$ . where: . $ ||x_i-v_j||^2 $= Euclidean distance between two pints x_i and v_j . $ c_i $ = number of data points in cluster i . $ k $ = number of centroids . To sum up the process of the algorithm: . Initialize K cluster centroids: Select K data points by random sampling without replacement | Calculate the centroid of the clusters: . $$ v_i = sum limits_{j=1}^{k}{ frac{x_j}{c_i}} $$ . | Calculate the sum of the squared distances between the data points and all cluster centroids | Relocate the data points to the closest centroid | Recalculate the centroid of the clusters which were changed . | Continue step 3-5 until no further changes in the clusters occur . | from sklearn.cluster import KMeans import pandas as pd kmeans = KMeans(n_clusters=3,init = &#39;k-means++&#39;, max_iter = 100, n_init = 10, random_state = 0) #Applying Kmeans classifier clusters = kmeans.fit_predict(data) plt.scatter(data[clusters == 0, 0], data[clusters == 0, 1], label = &#39;Type 1&#39;) plt.scatter(data[clusters == 1, 0], data[clusters == 1, 1], label = &#39;Type 2&#39;) plt.scatter(data[clusters == 2, 0], data[clusters == 2, 1], label = &#39;Type 3&#39;) plt.title(&#39;Clusters&#39;) plt.show() . Elbow Method: Finding the best K . The elbow method is a good way to find an ideal K value (number of clusters). Unlike hierarchical clustering, this is an important point because we have to define a good number of clusters to obtain good results with K-Means clustering where we need a predefined K value. . Elbow method works buy plotting the sum of squared distances with respect to the numbers of clusters (K). The ideal K numbers would be where the curve starts to flatten out (where it forms an elbow). . k_values = list(range(1, 10)) # define the range of K values that we want to examine sum_sq_dist = [] for k in k_values: k_m = KMeans(n_clusters=k) k_m.fit(data) sum_sq_dist.append(k_m.inertia_) # inertia_: Sum of squared distances of samples to their closest cluster center # Plot sum squared distances with respect to K plt.figure(figsize=(5, 5)) plt.plot(k_values, sum_sq_dist, &#39;-o&#39;) plt.xlabel(&#39;Number of clusters (K)&#39;) plt.ylabel(&#39;Sum of squared distances&#39;) plt.axvline(x=3, c=&#39;r&#39;, linestyle=&#39;--&#39;) . &lt;matplotlib.lines.Line2D at 0x7fc36f59e610&gt; . The elbow method also confirms that 3 clusters would be an ideal number. If we did not have a hint about the logical number of clusters from the iris dataset, using the elbow method would be a good idea to determine the ideal number of clusters. . Density Model: DBSCAN . In this section, we will talk about Density-based Spatial Clustering of Applications with Noise (DBSCAN). We have talked about K-Means clustering in the previous section. While K-Means clustering is easy to apply, it does not deal well with outliers due to the way it constructs the clusters. It assigns all the points to a cluster in which outliers are also assigned to clusters in which they may not belong. This a big problem for anomaly detection because anomalous points will also be assigned to a cluster and they will also pull the centroid of the clusters towards them, making it even harder to identify them as anomalies. . This is where density based clustering run to our help. In comparison to centroid based clustering like K-Means clustering, density based models identify the dense areas of cluster points, making it possible to learn clusters of arbitrary shape and therefore identify outliers/anomalies in the dataset. . Example for spherical or convex shaped clusters where K-means and hierarchical clustering would work well (well seperated clusters are also visibly identifiable). . from sklearn.datasets import make_blobs X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=0) plt.plot(X[:,0], X[:,1], &#39;o&#39;, markerfacecolor=&#39;b&#39;, markeredgecolor=&#39;k&#39;, markersize=6) . [&lt;matplotlib.lines.Line2D at 0x7fc36f5c8f50&gt;] . Example for a dataset with arbitrary shape and a lot of noise: . from sklearn.datasets import make_moons moons_X, moon_y = make_moons(n_samples = 300, noise= 0.15, random_state=0) plt.plot(moons_X[:,0], moons_X[:,1], &#39;o&#39;, markerfacecolor=&#39;b&#39;, markeredgecolor=&#39;k&#39;, markersize=6) . [&lt;matplotlib.lines.Line2D at 0x7fc36d4a8b90&gt;] . Important Parameters . DBSCAN algorithm works on two important parameters: . Epsilon (ϵ): This parameters determines the distance around a data point which defines the neighborhood of that point. If the distance between two points is less than this value, then they will be considered as neighbors. This parameter is important when it comes to determining the outliers. If Epsilon is chosen too small, then most of the data will be considered as outliers. If it is chosen too large, then many clusters will merge together forming a large clusters including majority of the data points. . How do we decide which Epsilon value is the best? We can use K-distance graph to find the best value. . . Minimum number of samples: This parameters determines the minimum number of datapoints should be inside the neighborhood defined using epsilon value. This parameter should be chosen larger for larger datasets. Generally this parameter can be determined from the number of dimensions (D) in data. It should be larger or equal to dimensionality of the dataset. . The three types of data points that exist in this model are: . Core Point: A data point which has more than the minimum number of samples parameter in its neighborhood. . Border Point: A data point which has points less than the minimum number of samples parameter within its neighborhood but it is in the epsilon range (neigborhood) of another core point. . Noise / Outlier Point: A data point which is neither a core point or a border point. . In the figure below, we can see that a potential candidate for a core point, can not be a core point when the minimum samples parameter is selected as four. It only has three points (including itself= within its neighborhood and therefore does not meet the threshold requirement. However the point below seems satisfy the requirement with 6 points within its neighborhood. . DBSCAN model steps: . Select a random data point from the data | Determine the number of points within its neighborhood using epsilon. If it is more than than the minimum samples parameter, assign it as a cluster. If less, than as noise/outlier point | We add the neighborhood as a part of the cluster. This results in neighborhood of each data point in the cluster also adding to the cluster distance. This way the cluster enlarges with new data points as their neighborhood (epsilon) is also added to the cluster until no further data points can be added to the cluster | The steps 2-4 are repeated until all data points have been selected and labeled | Reference . Reference . from sklearn.cluster import DBSCAN db = DBSCAN(eps = 0.2, min_samples=7).fit(moons_X) clusters = db.labels_ . plt.scatter(moons_X[clusters == 0, 0], moons_X[clusters == 0, 1], label = &#39;Type 1&#39;) plt.scatter(moons_X[clusters == 1, 0], moons_X[clusters == 1, 1], label = &#39;Type 2&#39;) plt.scatter(moons_X[clusters == 2, 0], moons_X[clusters == 2, 1], label = &#39;Type 3&#39;) plt.scatter(moons_X[clusters == -1, 0], moons_X[clusters == -1, 1], label = &#39;Outlier&#39;) plt.title(&#39;Clusters&#39;) plt.show() . We have talked enough about the DBSCAN model and its concept. We can use the DBSCAN model from Sklearn to cluster the iris dataset and compare its performance to other clustering algorithms. . db = DBSCAN(eps = 0.4, min_samples=4).fit(data) # we can play with the epsion variable and see how choosing a large value creates # a single large cluster and choosing a small value, creates small clusters with many outliers clusters = db.labels_ . plt.scatter(data[clusters == 0, 0], data[clusters == 0, 1], label = &#39;Type 1&#39;) plt.scatter(data[clusters == 1, 0], data[clusters == 1, 1], label = &#39;Type 2&#39;) plt.scatter(data[clusters == 2, 0], data[clusters == 2, 1], label = &#39;Type 3&#39;) plt.scatter(data[clusters == -1, 0], data[clusters == -1, 1], label = &#39;Outlier&#39;) plt.title(&#39;Clusters&#39;) plt.show() . How to select Epsilon? . We can use the K-distance graph which we previously mentioned to find an ideal epsilon value. K-distance graph shows us the distance between each datapoint and its K nearest neigbors. For this we have to calculate the Nearest Neighbors and their distance using the NearestNeighbors from sklearn. The average distances are then plotted against the number of points used to calculate the distance. The most optimal value as epsilon is the distance where the graph has the highest curvature (at the elbow). Looking at the graph we can easily intrepret that this point signals us the point where the density border of the cluster lies. After this point, the average distance increases greatly with fewer number of points, because they lay farther than the rest of the points (potential outliers). . This point represents the optimization point where diminishing returns are no longer worth the additional cost. This concept of diminishing returns applies here because while increasing the number of clusters will always improve the fit of the model, it also increases the risk that overfitting will occur. . Reference . Let&#39;s calculate the ideal epsilon for iris dataset. We can use the min samples value that we have used as the value for number of neighbors. . from sklearn.neighbors import NearestNeighbors neighbors = NearestNeighbors(n_neighbors=4) neighbors_fit = neighbors.fit(data) distances, indices = neighbors_fit.kneighbors(data) . import numpy as np distances = np.sort(distances, axis=0) distances = distances[:,1] plt.plot(distances) . [&lt;matplotlib.lines.Line2D at 0x7fc36d5154d0&gt;] . As seen, the highest curvature is reached at average distance 0.4, which would also the epsilon which we used previously. .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/11/06/Clustering.html",
            "relUrl": "/jupyter/2022/11/06/Clustering.html",
            "date": " • Nov 6, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Evaluation Methods for Classification Problems",
            "content": "Confusion Matrix . Confusion Matrix is a table where we can clearly see the predicted categories and actual categories. The confusion matrix for a binary classification looks like this: . . True positives (TP): Predicted positive and are actually positive. . False positives (FP): Predicted positive and are actually negative. . True negatives (TN): Predicted negative and are actually negative. . False negatives (FN): Predicted negative and are actually positive. . To calculate the confusion matrix, we will be using the ConfusionMatrixDisplay class from the sklearn library. . Documentation for ConfusionMatrixDisplay . from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import ConfusionMatrixDisplay from sklearn.linear_model import LogisticRegression X,y = load_breast_cancer(return_X_y = True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = LogisticRegression(max_iter=10000) clf.fit(X_train, y_train) ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels = [&#39;malignant&#39;, &#39;benign&#39;]) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fc220ad5d10&gt; . Depending on some problems, it can be very important to see the number of false negatives or false positives. This is where the confusion matrix comes in very handy because it does exactly that. . In this case, we have the precision matrix for the cancer dataset. We can see that the number of true positives is 39. That means the model was able to identify 39 samples with cancer correctly. . However, we also have 1 false positive (the patient actually does not have cancer, but the model says the patient has cancer). False positives in this case may not be so critical, since the additional medical controls will later reveal that the patient does not have cancer (except for a few days of difficult days for the patient...). . What is here very critical is the 4 patients who were predicted as cancer-free but they do actually have cancer (false negatives). This is a case we do not want to have at all, since the lives of cancer patients are at stake here. Therefore, for this problem, we would aim for least possible number of false negatives. For another problem, reducing the number of false positives could be more important. This is why the confusion matrix is very important when it comes to classification problems. . Confusion Matrix for Multi-class Classification . The confusion matrix becomes larger as we move form a binary classification problem to a multi-class classification problem. This is how a confusion matrix for a multi-class problem would look like: . . The size of the confusion matrix is same as the number of target categories. One big advantage of confusion matrix is that, it allows us to see how many samples of which class were misclassified, and which predicted class was confused with which actual class. . Accuracy . Accuracy is a measure of how often the classifier makes a correct prediction. It is the ratio of total correct predictions to the total predictions. . $$ text{Accuracy} = frac{ text{TP + TN}}{TP + TN + FP + FN} $$ . Accuracy is a easy to understand metric and can be very useful when the dataset is balanced. However, it can be misleading when evaluating imbalanced datasets. When target classes in dataset have a large differences in terms of samples, it is an imbalanced dataset. Accuracy is not the best metric to use when dealing with imbalanced datasets. Let&#39;s have a look at the example below. . . When we look at the confusion matrix carefully, we see that we have only 25 patients who actually have cancer, in comparison to 375 who do not actually have cancer. The model performs pretty well with an accuracy of 0.95, or is it? Not really, the model is pretty good at predicting the patients who do not have cancer, but we can not say the same for the patients who actually have cancer (only predicted 10 out of 25 who actually have cancer correctly). Because the accuracy only considers the ratio of correct predictions to total predictions, the most frequent class performance dominates the metric, leading to misleading interpretation of model performance when we have an imbalanced dataset. . To calculate accuracy we will be using the accuracy_score function from the sklearn library. . Documentation for accuracy_score() . y_pred = clf.predict(X_test) #calling the accuracy_score function from metrics accuracy_score = metrics.accuracy_score(y_test, y_pred) print(&quot;Accuracy score is: {:.2f}&quot;.format(accuracy_score)) . Accuracy score is: 0.96 . correct_predictions = [pred for i,pred in enumerate(y_pred) if pred == y_test[i]] accuracy_manual = len(correct_predictions)/len(y_pred) print(&quot;Accuracy score is: {:.2f}&quot;.format(accuracy_manual)) . Accuracy score is: 0.96 . Precision . Precision is the measure of the ratio of true positive predictions to the total positive predictions of the model. Precision is a useful metric in cases where false positives are more significant than false negatives. SUch cases can be recommendation systems where a false positives can lead to customer churn which in turn reduces business success. . $$ text{Precision} = frac{ text{TP}}{TP + FP} $$ . . The precision from our previous example would be: $$ text{Precision} = frac{10}{10 + 5} = 0.667 $$ . Not as good as the accuracy right? In this case false negatives are more important than false positives, we do not want to misdiagnose any cancer patient as negative if they actually have cancer! We also a have a metric for that, Recall! . To calculate accuracy we will be using the precision_score function from the sklearn library. . Documentation for precision_score() . precision_score = metrics.precision_score(y_test, y_pred) print(&quot;Precision score is: {:.2f}&quot;.format(precision_score)) . Precision score is: 0.95 . TP = [pred for i,pred in enumerate(y_pred) if (pred == y_test[i] &amp; pred == 1)] TP_FP = [pred for i,pred in enumerate(y_pred) if (pred == 1)] precision_manual = len(TP) / len(TP_FP) print(&quot;Precision score is: {:.2f}&quot;.format(precision_manual)) . Precision score is: 0.95 . Recall . Recall is the ratio of how many of the positive cases the model was able to correctly predict. Recall is a useful metric when false negatives are more significant than false positives. Like we mentioned, it is, for example, important in medical cases (like we have with cancer dataset) where we do not want to miss any actual positives whereas false alarms (false positives) can be considered less important. . $$ text{Recall} = frac{ text{TP}}{TP + FN} $$ . . The recall from our previous example would be: $$ text{Recall} = frac{10}{10 + 15} = 0.4 $$ . As we can see our recall is much morse compared to accuracy and precision, which, in this case is actually the most important metric. Here we can clearly see how using accuracy for imbalanced datasets can be very misleading. . To calculate accuracy we will be using the recall_score function from the sklearn library. . Documentation for recall_score() . recall_score = metrics.recall_score(y_test, y_pred) print(&quot;Recall score is: {:.2f}&quot;.format(recall_score)) . Recall score is: 0.99 . TP = [pred for i,pred in enumerate(y_pred) if (pred == y_test[i] &amp; pred == 1)] TP_FN = [pred for i,pred in enumerate(y_pred) if (y_test[i] == 1)] recall_manual = len(TP) / len(TP_FN) print(&quot;Precision score is: {:.2f}&quot;.format(recall_manual)) . Precision score is: 0.99 . F1-Score . F1- score is a combination of both precision and recall. It is the harmonic mean of precision and recall and is maximum when precision is equal to recall. So when we are looking for the best of both worlds (when we have no difference of significance between false positives and false negatives) f1 is our metric. . $$ text{F1-Score} = frac{2 * text{Precision}* text{Recall}}{Precision + Recall} $$ . The F1-score from our previous example would be: $$ text{F1-Score} = frac{2 *0.667*0.4}{0.667 + 0.40} = 0.5 $$ . F1-score penalizes the extreme values more and is an effective metric when adding more data with no effect on outcome or when we have a high number of true negatives. It is also a better option to use compared to accuracy when we are dealing with imbalanced data. . To calculate accuracy we will be using the f1_score function from the sklearn library. . Documentation for f1_score() . f1_score = metrics.f1_score(y_test, y_pred) print(&quot;F1-score is: {:.2f}&quot;.format(f1_score)) . F1-score is: 0.97 . f1_manual = 2*precision_manual*recall_manual/(precision_manual + recall_manual) print(&quot;F1-score is: {:.2f}&quot;.format(f1_manual)) . F1-score is: 0.97 . Precision-Recall Curve ( PR Curve) . PR curve is a graph illustrating the change of precision values with respect to the recall value for binary classification. PR curve is plotted by calculating the precision and recall values for different thresholds for a classifier and plotting the values. Ex. for logistic regression the threshold would be the predicted probability of a prediction belonging to the positive class. . It is often used in problems with imbalanced dataset. We have the precision values on the y-axis and the recall values on the x-axis. We saw in earlier examples for which use cases precision and recall metrics maybe suitable. Well, the PR curve allows us to consider both metrics and the change of one with respect to other. . Considering both precision and recall can be useful when we have an imbalance of samples between the two target classes. This is specially the case, when samples of negative class ( class 0) are more than then samples of positive class (class 1). Both precision and recall do not consider the number of true negatives and focus on the correct of false prediction of the positive class. . To plot the pr-curve for a classifier, we will be using the precision_recall_curve() function from the sklearn library. . Documentation for precision_recall_curve() . We can obtain the probabilities using the predict_proba() method, and then use these probabilities to construct the PR curve. The decision threshold is usually 0.5. This means if probability is larger than 0.5, then it will be predicted as positive. . from sklearn.metrics import precision_recall_curve import matplotlib.pyplot as plt %matplotlib inline probabilities = clf.predict_proba(X_test)[:,1] precision_vals,recall_vals,thresholds = precision_recall_curve(y_test, probabilities) plt.plot(recall_vals, precision_vals, label = &#39;PR Curve&#39;, marker=&#39;o&#39;) for i,val in enumerate(precision_vals): if precision_vals[i] != precision_vals[-1]: plt.text(recall_vals[i], val, str(round(thresholds[i],3)), fontsize=10) plt.axvline(x=1,ymin=0, ymax=0.95, color=&#39;r&#39;, label=&quot;Best Model&quot;) plt.axhline(y=1, xmin=0, xmax=0.95, color=&#39;r&#39;) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) plt.legend([&#39;PR Curve&#39;, &#39;Best model&#39;], loc = &#39;best&#39;) plt.show() . Here we have calculated the precision and recall values for each threshold and plotted them. The red lines represent what a perfect classifier would look like. Our PR curves also seems to be performing pretty well. On each marker we can also see the associated threshold probability value. We want to choose the threshold which lies closer the our red line of perfection. Here depending which metric we are aiming to maximimize, we can choose the threshold according to that. If we have no preference for recall or precision, We can also calculate the f1-score for each threshold and make a selection according to that. . f1_score = (2 * precision_vals * recall_vals) / (precision_vals + recall_vals) f1_score . array([0.96598639, 0.95890411, 0.96551724, 0.97222222, 0.97902098, 0.97183099, 0.9787234 , 0.98571429, 0.97841727, 0.97101449, 0.96350365, 0.95588235, 0.94814815, 0.94029851, 0.93233083, 0.92424242, 0.91603053, 0.90769231, 0.89922481, 0.890625 , 0.88188976, 0.87301587, 0.864 , 0.85483871, 0.84552846, 0.83606557, 0.82644628, 0.81666667, 0.80672269, 0.79661017, 0.78632479, 0.77586207, 0.76521739, 0.75438596, 0.74336283, 0.73214286, 0.72072072, 0.70909091, 0.69724771, 0.68518519, 0.6728972 , 0.66037736, 0.64761905, 0.63461538, 0.62135922, 0.60784314, 0.59405941, 0.58 , 0.56565657, 0.55102041, 0.53608247, 0.52083333, 0.50526316, 0.4893617 , 0.47311828, 0.45652174, 0.43956044, 0.42222222, 0.40449438, 0.38636364, 0.36781609, 0.34883721, 0.32941176, 0.30952381, 0.28915663, 0.26829268, 0.24691358, 0.225 , 0.20253165, 0.17948718, 0.15584416, 0.13157895, 0.10666667, 0.08108108, 0.05479452, 0.02777778, 0. ]) . import numpy as np ix = np.argmax(f1_score) print(&#39;Best Threshold=%f, F1-Score=%.3f&#39; % (thresholds[ix], f1_score[ix])) . Best Threshold=0.851244, F1-Score=0.986 . ROC Curve (Receiver Operating Characteristic Curve) . A ROC curve is a plot indicating the performance of a classification model for different classification thresholds by plotting True Positive Rate (Recall) against the False Positive Rate (FPR). It has a similar idea to PR Curve but we plot the FPR instead of precision in this curve. . $$ text{FPR} = frac{ text{FP}}{FP + TN} $$ . To plot the ROC-curve for a classifier, we will be using the roc_curve() function from the sklearn library. . Documentation for roc_curve() . from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_test, probabilities) plt.plot( fpr, tpr, label = &#39;ROC Curve&#39;, marker=&#39;o&#39;) for i,val in enumerate(tpr): plt.text(fpr[i], val, str(round(thresholds[i],3)), fontsize=10) plt.xlabel(&#39;FPR&#39;) plt.ylabel(&#39;TPR&#39;) plt.legend(loc = &#39;best&#39;) plt.show() . By lowering the classification threshold, more predictions are identified as positive. This increases both false positivi and true positive rates. Therefore, the curve looks different to that of a PR curve. . In ROC curve, we are looking for the ideal point which is closest to the top left corner. Having a number to estimate how close our curve is to the top left corner would be useful right? Area Under the Curve (AUC) is another method which can help us here. . Area Under the Curve (AUC) . AUC / Area Under the Curve is the entire two dimensional area under the ROC curve. AUC helps us summarise the information from a ROC curve (can also be used for PR curve) into a single value. . ROC curve is basically a curve of probabilities (threholds) and AUC basically is the measure of separability of the calsses. It tells us how well the model is able to predict the classes. Therefore, a higher AUC value means a better classifier. AUC value varies between 0 and 1, meaning a value of 1 would mean a perfect classifier. . To calculate the AUC score for roc, we will use the roc_auc_score() function from sklearn library. . Documentation for roc_auc_score() . Before continuing, let&#39;s plot our ROC-curve again to examine the area under the curve visually. . plt.plot( fpr, tpr, label = &#39;ROC curve&#39;) plt.fill_between(fpr, tpr, alpha=0.3) plt.xlabel(&#39;FPR&#39;) plt.ylabel(&#39;TPR&#39;) plt.legend(loc = &#39;best&#39;) plt.grid() plt.show() . A classifier model which can not discriminate between the two classes (a no-skill classifier) would predict the most frequent occuring class or a random class for all predictions and it would have an AUC of 0.5. Its ROC curve would be a diagonal line as below. A classifier which predicts wrong all the time (0% correct prediction) would look exactly opposite of the best curve. . from sklearn.metrics import roc_curve pnt_1 = [0, 1] pnt_2 = [0, 1] fpr, tpr, thresholds = roc_curve(y_test, probabilities) plt.plot( fpr, tpr, label = &#39;ROC curve for DecisionTree&#39;) plt.plot( pnt_1, pnt_2, label = &#39;ROC curve for AUC=0.5&#39;, color=&#39;r&#39;) plt.fill_between( pnt_1, pnt_2, alpha=0.3, facecolor=&#39;r&#39;) plt.axvline(x=1,ymin=0.05, ymax=0.95, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&quot;ROC curve for AUC=0.0&quot;) plt.axhline(y=0, xmin=0.05, xmax=0.95, color=&#39;r&#39;, linestyle=&#39;--&#39;) plt.xlabel(&#39;FPR&#39;) plt.ylabel(&#39;TPR&#39;) plt.legend(loc = &#39;best&#39;) plt.show() . We can use the roc_auc_score function from the sklearn library to calculate the ROC AUC score for the plot above . from sklearn.metrics import roc_auc_score roc_auc_score(y_test, probabilities) . 0.9977071732721913 . Advantages: . AUC is invariant to scale. It is a measure of how good the predictions are ranked, rather than their absolute values. | AUC is invariant to classification-threshold. It is a measure of the model&#39;s performance (correctness of predictions) irrespective of the classification threshold chosen. | . Disadvantages: . Invariance to scale is not always wanted. Sometimes we need the well adapted oribability outputs. | Invariance to threshold is not always wanted as well. When there are big differences in terms of false negatives and false positives, we may want to aim to minimize only one of these. For example, in out cancer example previously, we mentioned how important it is to minimize the false negatives. In this case, AUC will not be a usefull metric. | . PR Curve vs ROC Curve . ROC curves are suitable when we are dealing with more of a balanced dataset, whereas PR curves are suitable for imbalanced datasets. . | The PR cuve does not consider the true negatives, therefore it is a useful metric when we have an imbalanced dataset, if the negative class is the majority class, considering making correct negative predictions are not that critical for the problem. . | Since ROC curve also takes the true negatives into consideration, it is a better metric to use if the when also the negative class is important (when both TP and TN are important). | .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/11/06/Classification-Evaluation.html",
            "relUrl": "/jupyter/2022/11/06/Classification-Evaluation.html",
            "date": " • Nov 6, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Linear Regression in Machine Learning",
            "content": "Content prepared by: Berk Hakbilen . What is Regression? . In statistical modeling, regression analysis estimates the relationship between one or more independent variables and the dependent variable which represents the outcome. . To explain with an example you can imagine a list of houses, with information regarding to the size, distance to city center, garden (independent variables). Using these information, you can try to understand how the price(dependent variables) changes. . So for a regression analysis we have a set of observations or samples with one or more variables/features. Then, we define a dependent variable (the outcome) and try to find a relation between the dependent variables and the independent variables. The best way to do this is by finding a function which best represent the data. . Linear Models for Regression . Linear Regression . In the linear regression model, we will use regression analysis to best represent the dataset through a linear function. Then, we will use this function to predict the outcome of a new sample/observation which was not in the dataset. . Linear regression is one of the most used regression models due to its simplicity and ease of understanding the results. Let&#39;s move on to the model formulations to understand this better. . . The linear regression function is written assuming a linear relationship between the variables: . $$y = w_1x_1 + ... + w_nx_n + c$$ . where w terms are the regression coefficients, x terms are the independent variables or features, y is dependent variable/outcome and c is the constant bias term. . We can write a simple linear regression function for the houses examples we mentioned above. . $$y_{price} = 500X_{size} - 350X_{distance to city} + 400.000$$ So if we plug in the features of a new house into this function, we can predict its price (let&#39;s assume size is 150m2 and distance to city center is 5 km). $$y_{price} = 500*150 - 350*5 + 400.000 = 75.000 - 1750 + 400.000 = 476.750$$ . See how the coefficient of distance to city center is minus. Meaning closer to center, more expensive the house will be. . We can create a simple fake regression dataset with only one feature and plot it to see the data behaviour more clearly. . import matplotlib.pyplot as plt from sklearn.datasets import make_regression plt.figure() plt.title(&#39;Samples in a dataset with only one feature (dependent variable)&#39;) X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 40, random_state=42) plt.scatter(X, y, marker= &#39;o&#39;, s=50) plt.show() . The dataset above has only one dependent variable. In this case, the regression function would be: $$y = w_1x_1 + c$$ . where w1 would be the slope the curve and c would be the offset value. . When we train our model on this data, the coefficients and the bias term will be determined automatically so that the regression function best fits the dataset. . The model algorithm finds the best coefficients for the dataset by optimizing an objective function, which in this case would be the loss function. The loss function represents the difference between the predicted outcome values and the real outcome values. . Least-Squared Linear Regression . In the Least-Squared linear regression model the coefficients and bias are determined by minimizing the sum of squared differences (SSR) for all of the samples in the data. This model is also called Ordinary Least-Squares. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2}$$ . If we interpret the function, it is a function determined by taking the square of the difference between the predicted outcome value and the real outcome value. . Let&#39;s train the Linear Regression model using the fake dataset we previously created and have a look at the calculated coefficients. . from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42) model = LinearRegression() model.fit(X_train, y_train) print(&#39;feature coefficient (w_1): {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_train, y_train))) print(&#39;R-squared score (test): {:.3f}&#39; .format(model.score(X_test, y_test))) . feature coefficient (w_1): [101.41780398] intercept (c): 53.342 R-squared score (training): 0.891 R-squared score (test): 0.735 . Here, R^2 is the coefficient of determination. This term represents the amount of variation in outcome(y) explained by the dependence on features (x variables). Therefore, a larger R^2 indicates a better model performance or a better fit. . When R^2 is equal to one, then RSS is equals to 0. Meaning the predicted outcome values and the real outcome values are exactly the same. We will be using the R^2 term to measure the performance of our model. . plt.figure(figsize=(6,5)) plt.scatter(X, y, marker= &#39;o&#39;, s=50, alpha=0.7) plt.plot(X, model.coef_*X + model.intercept_, &#39;r-&#39;) plt.title(&#39;Least-squares linear regression model&#39;) plt.xlabel(&#39;Variable/feature value (x)&#39;) plt.ylabel(&#39;Outcome (y)&#39;) plt.show() . Ridge Regression - L2 Regularization . Ridge regression model calculates coefficients and the bias (w and c) using the same criteria in Least-Squared however with an extra term. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2} + boxed { alpha sum limits_{j=1}^{m} {w_j^2}}$$ . This term is a penalty to adjust the large variations in the coefficients. The linear prediction formula is still the same but only the way coefficients are calculated differs due to this extra penalty term. This is called regularization. It serves to prevent overfitting by restricting the variation of the coefficients which results in a less complex or simpler model. . This extra term is basically the sum of squares of the coefficients. Therefore, when we try to minimize the RSS function, we also minimize the the sum of squares of the coefficients which is called L2 regularization. Moreover, the alpha constant serves to control the influence of this regularization. This way, in comparison to the Least-Squared model, we can actually control the complexity of our model with the help of alpha term. The higher alpha term, higher the regularization is, and simpler the model will be. . The accuracy improvement with datasets including one dependent variable (feature) is not significant. However, for datasets with multiple features, regularization can be very effective to reduce model complexity, therefore overfitting and increase model performance on test set. . Let&#39;s have a look at its implementation in python. . from sklearn import datasets X,y = datasets.load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42) from sklearn.linear_model import Ridge model = Ridge() model.fit(X_train, y_train) print(&#39;feature coefficients: {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) . feature coefficients: [ 50.55155508 -67.72236516 278.3007281 197.62401363 -6.2459735 -26.22698465 -151.39451804 120.32319558 215.85469359 101.75554294] intercept (c): 152.514 . plt.figure(figsize=(10,5)) alphas = [1,5,10,20,50,100,500] features = [&#39;w_&#39;+str(i+1) for i,_ in enumerate(model.coef_)] for alpha in alphas: model = Ridge(alpha=alpha).fit(X_train,y_train) plt.scatter(features,model.coef_, alpha=0.7,label=(&#39;alpha=&#39;+str(alpha))) plt.axhline(0) plt.xticks(features) plt.legend(loc=&#39;upper left&#39;) plt.show() . Normalization can be applied unfairly to the features when they have different scales (when one feature has values around 0-1 and the other has from 100-1000). This can cause inaccuracies in our model when we apply regularization. In this case, feature scaling comes to our help to normalize all the values in the dataset, so that we can get rid of the scale differences. We will look in to feature scaling in another section... . Lasso Regression - L1 Regularization . Lasso regression is also a regularized linear regression model. In comparison to Ridge regression, it uses L1 regularization as the penalty term while calculating the coefficients. . Let&#39;s have a look at how the RSS function looks like with the penalty term for L1 regularization. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2} + boxed { alpha sum limits_{j=1}^{m} {|w_j|}}$$ . The penalty term for L1 regularization is the sum of absolute values of the coefficients. Therefore, when the algorithm tries to minimize RSS, it enforces the regularization by minimizing the sum of absolute values of the coefficients. . This results in coefficients of the least effective paramaters to be 0 which is kind of like feature selection. Therefore, it is most effectively used for datasets where there a few features with a more dominant effect compared to others. This results in eliminating features which have a small effect by setting their coefficients to 0. . Alpha term is again used to control the amount of regularization. . from sklearn.linear_model import Lasso model = Lasso() model.fit(X_train, y_train) print(&#39;feature coefficients: {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) . feature coefficients: [ 0. -0. 398.38436775 46.17884277 0. 0. -0. 0. 238.18740159 0. ] intercept (c): 152.944 . After finding the coefficients of the dominant features, we can go ahead and list their labels. . import numpy as np data = datasets.load_diabetes() np.take(data.feature_names,np.nonzero(model.coef_)) . array([[&#39;bmi&#39;, &#39;bp&#39;, &#39;s5&#39;]], dtype=&#39;&lt;U3&#39;) . alphas = [0.1,0.5,1,2,5,10] for alpha in alphas: model = Lasso(alpha=alpha).fit(X_train,y_train) print(&#39;feature coefficients for alpha={}: n{}&#39;.format(alpha,model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_train, y_train))) print(&#39;R-squared score (test): {:.3f} n&#39; .format(model.score(X_test, y_test))) . feature coefficients for alpha=0.1: [ 0. -151.41550421 540.85933829 337.22472854 -85.19030645 -0. -262.90345036 0. 418.24822392 9.92430726] R-squared score (training): 0.508 R-squared score (test): 0.502 feature coefficients for alpha=0.5: [ 0. -0. 492.28522027 187.79307581 -0. -0. -107.63391392 0. 322.2269439 0. ] R-squared score (training): 0.451 R-squared score (test): 0.477 feature coefficients for alpha=1: [ 0. -0. 398.38436775 46.17884277 0. 0. -0. 0. 238.18740159 0. ] R-squared score (training): 0.347 R-squared score (test): 0.379 feature coefficients for alpha=2: [ 0. 0. 75.24915126 0. 0. 0. -0. 0. 0. 0. ] R-squared score (training): 0.052 R-squared score (test): 0.041 feature coefficients for alpha=5: [ 0. 0. 0. 0. 0. 0. -0. 0. 0. 0.] R-squared score (training): 0.000 R-squared score (test): -0.014 feature coefficients for alpha=10: [ 0. 0. 0. 0. 0. 0. -0. 0. 0. 0.] R-squared score (training): 0.000 R-squared score (test): -0.014 . Ridge or Lasso? . To sum up, it makes sense to use the Ridge regression model there are many small to medium effective features. If there are only a few dominantly effective features, use the Lasso regression model. . Polynomial Regression . Linear regression performs well on the assumption that the relationship between the independent variables (features) and the dependent variable(outcome) is linear. If the distrubtion of the data is more complex and does not show a linear behaviour, can we still use linear models to represent such datasets? This is where polynomial regression comes in very useful. . To capture this complex behaviour, we can add higher order terms to represent the features in the data. Transforming the linear model with one feature: . $$y = w_1x + c rightarrow boxed{y=w_1x + w_2x^2 + c} $$ . Since the coefficients are related to features linearly, this is still a liner model. However, it contains quadratic terms and the curve fitted is a polynomial curve. . Let&#39;s continue with an example for Polynomial regression. To convert the features to higher order terms, we can use the PolynomialFeatures class from scikit-learn. Then we can use the Linear regression model from before to train the model. . But before, let us create a dataset which could be a good fit for a 2nd degree function. For that we will use numpy to create random X points and plug them into a representative function. . np.random.seed(0) X = 2 - 3 * np.random.normal(0, 1, 100) y = X - 2 * (X ** 2) + np.random.normal(-3, 3, 100) plt.scatter(X, y, s=10) plt.show() . We can reshape the arrays we created so that we can feed them in to the model. First, we will train a LinearRegression model to see how it fits to this data. . X = X[:, np.newaxis] y = y[:, np.newaxis] model = LinearRegression() model.fit(X,y) print(&#39;feature coefficients: n{}&#39;.format(model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X, y))) plt.plot(X, model.coef_*X + model.intercept_, &#39;r-&#39;) plt.scatter(X,y, s=10) plt.show() . feature coefficients: [[-6.36572661]] R-squared score (training): 0.397 . As expected, Linear Regression model does not provide a very good fit with the normal features for a dataset of this behaviour. Now, we can create 2nd order Polynomial features using the PolynomialFeatures class from sk-learn library. Then, use these new 2nd order features to train the same linear regression model. . from sklearn.preprocessing import PolynomialFeatures poly_features= PolynomialFeatures(degree=2) X_poly = poly_features.fit_transform(X) model = LinearRegression() model.fit(X_poly,y) print(&#39;feature coefficients: n{}&#39;.format(model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_poly, y))) plt.scatter(X,model.predict(X_poly),s=10,label=&quot;polynomial prediction&quot;) plt.scatter(X,y,s=10,label=&quot;real data&quot;) plt.legend(loc=&#39;lower left&#39;) plt.show() . feature coefficients: [[ 0. 0.96597113 -2.02225052]] R-squared score (training): 0.990 . This time, we were able to obtain a very good fit using the same linear regression model but with 2nd order features obtained from the PolynomialFeatures class. This is a perfect example to show how Polynomial Linear Regression can be used to obtain better fits with data which do not have a linear relationship between the features and the outcome value. .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/03/19/Linear-Regression.ipynb.html",
            "relUrl": "/jupyter/2022/03/19/Linear-Regression.ipynb.html",
            "date": " • Mar 19, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey there👋 My name is Berk Hakbilen. I am currently working as a data scientist. I transitioned into a career into data science after discovering my passion for data and developing myself in the field of data science as a hobby. During this transition, free online material available was a huge help for me. That’s why I decided to create this blog, to publish some of the free learning material that I have created in my free time, so that it can be helpful for others who are interested about data science! . I hope this material is useful for you. If you have any questions or feedback, feel free to contact me! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hakbilenberk.github.io/LearningML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hakbilenberk.github.io/LearningML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}