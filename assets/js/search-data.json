{
  
    
        "post0": {
            "title": "Title",
            "content": "Content prepared by: Berk Hakbilen . Logistic Regression Theory . Logistic Regression for Classification . Despite its name, logistic regression is a essential technique for binary classification and multiclass classification rather than a regression algorithm. It falls under the category of linear classifiers. It is a fast, and simple model, making it easier to interpret the results. . Logistic regression is like its name suggests also a regression analysis. However, unlike linear regression (which is not suitable for a classification analysis), the calculated result is the probability of an event. . Let&#39;s have a look at the linear regression model and then derive the logistic regression function from this. . Linear regression formula: $$y^i = a_0 + a_1x_1^i + .... + a_nx_n^i $$ . On the other hand the so called sigmoid function is: $$P(x) = frac{1}{1 + exp(-x)} $$ . and its curve: . Here we can see that its output is always a value ranging from 0 to 1 which is the exact behaviour we want for a binary classification. . Like we just mentioned, for a classification problem, we want to get probabilities between 0 - 1 as results. We can achieve that by substituting our linear regression function into our sigmoid function, we obtain our logistic regression function: $$P(y^i) = frac{1}{1 + exp(-(a_0 + a_1x_1^i + .... + a_nx_n^i))} $$ . Looking at the formula we can see that the regression coefficients are now superscript of the exponential term (e). This way the regression coefficients from the linear regression function still effect the probability outcome of the logistic function. . Exploratory Data Analysis . from sklearn.datasets import load_breast_cancer import pandas as pd cancer = load_breast_cancer() df = pd.DataFrame(cancer.data, columns=cancer.feature_names) df[&#39;result&#39;] = pd.Series(cancer.target) . df.head() . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension radius error texture error perimeter error area error smoothness error compactness error concavity error concave points error symmetry error fractal dimension error worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension result . 0 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | 1.0950 | 0.9053 | 8.589 | 153.40 | 0.006399 | 0.04904 | 0.05373 | 0.01587 | 0.03003 | 0.006193 | 25.38 | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | 0 | . 1 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | 0.5435 | 0.7339 | 3.398 | 74.08 | 0.005225 | 0.01308 | 0.01860 | 0.01340 | 0.01389 | 0.003532 | 24.99 | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | 0 | . 2 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | 0.7456 | 0.7869 | 4.585 | 94.03 | 0.006150 | 0.04006 | 0.03832 | 0.02058 | 0.02250 | 0.004571 | 23.57 | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | 0 | . 3 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | 0.2597 | 0.09744 | 0.4956 | 1.1560 | 3.445 | 27.23 | 0.009110 | 0.07458 | 0.05661 | 0.01867 | 0.05963 | 0.009208 | 14.91 | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | 0 | . 4 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | 0.1809 | 0.05883 | 0.7572 | 0.7813 | 5.438 | 94.44 | 0.011490 | 0.02461 | 0.05688 | 0.01885 | 0.01756 | 0.005115 | 22.54 | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 mean radius 569 non-null float64 1 mean texture 569 non-null float64 2 mean perimeter 569 non-null float64 3 mean area 569 non-null float64 4 mean smoothness 569 non-null float64 5 mean compactness 569 non-null float64 6 mean concavity 569 non-null float64 7 mean concave points 569 non-null float64 8 mean symmetry 569 non-null float64 9 mean fractal dimension 569 non-null float64 10 radius error 569 non-null float64 11 texture error 569 non-null float64 12 perimeter error 569 non-null float64 13 area error 569 non-null float64 14 smoothness error 569 non-null float64 15 compactness error 569 non-null float64 16 concavity error 569 non-null float64 17 concave points error 569 non-null float64 18 symmetry error 569 non-null float64 19 fractal dimension error 569 non-null float64 20 worst radius 569 non-null float64 21 worst texture 569 non-null float64 22 worst perimeter 569 non-null float64 23 worst area 569 non-null float64 24 worst smoothness 569 non-null float64 25 worst compactness 569 non-null float64 26 worst concavity 569 non-null float64 27 worst concave points 569 non-null float64 28 worst symmetry 569 non-null float64 29 worst fractal dimension 569 non-null float64 30 result 569 non-null int64 dtypes: float64(30), int64(1) memory usage: 137.9 KB . import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) sns.countplot(df[&#39;result&#39;]) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd242d02950&gt; . cancer.target_names . array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;) . benign, malignant = df[&#39;result&#39;].value_counts() print(&#39;Number of benign results {} corresponding to {} percent: &#39;.format(benign,round(benign / len(df) * 100, 2))) print(&#39;Number of malignant results {} corresponding to {} percent: &#39;.format(malignant,round(malignant / len(df) * 100, 2))) . Number of benign results 357 corresponding to 62.74 percent: Number of malignant results 212 corresponding to 37.26 percent: . cols = [&#39;result&#39;, &#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;] sns.pairplot(data=df[cols], hue=&#39;result&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x7fd2423c4990&gt; . f, ax = plt.subplots(figsize=(20, 20)) corr = df.corr().round(2) # Create a mask for the lower triangle import numpy as np mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # create the heatmap sns.heatmap(corr, mask=mask, square=True, annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd2348f62d0&gt; . Logistic Regression Model . from sklearn.model_selection import train_test_split y = df[&#39;result&#39;].values X = df.drop(&#39;result&#39;,axis=1).values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . X_train.shape . (455, 30) . from sklearn.linear_model import LogisticRegression . model = LogisticRegression(max_iter=10000) . logisticregression = model.fit(X_train,y_train) . print(&quot;training set score: %f&quot; % logisticregression.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression.score(X_test, y_test)) . training set score: 0.960440 test set score: 0.956140 . Because our training and test score are closer to each other, we can see that we are actually underfitting. Let&#39;s try a higher C value which means a more complex model which fits better to the data. . model = LogisticRegression(max_iter=10000,C=1000) logisticregression_1000 = model.fit(X_train,y_train) print(&quot;training set score: %f&quot; % logisticregression_1000.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression_1000.score(X_test, y_test)) . training set score: 0.986813 test set score: 0.991228 . Because our training and test score are closer to each other, we can see that we are actually underfitting. Let&#39;s try a higher C value which means a more complex model which fits better to the data. . model = LogisticRegression(max_iter=10000,C=0.1) logisticregression_0_1 = model.fit(X_train,y_train) print(&quot;training set score: %f&quot; % logisticregression_0_1.score(X_train, y_train)) print(&quot;test set score: %f&quot; % logisticregression_0_1.score(X_test, y_test)) . training set score: 0.949451 test set score: 0.964912 . A lower C does not create a big difference since our model is already underfitting to the data. . from sklearn.metrics import confusion_matrix y_pred = model.predict(X_test) cf_matrix = confusion_matrix(y_test, y_pred,labels=[0,1]) cf_matrix . array([[40, 3], [ 1, 70]]) . sns.heatmap(cf_matrix, annot=True, cmap=&#39;Blues&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f895589abd0&gt; . from sklearn.metrics import plot_confusion_matrix disp = plot_confusion_matrix(model, X_test, y_test, display_labels=[&#39;Benign&#39;,&#39;Malignant&#39;]) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator. warnings.warn(msg, category=FutureWarning) . from sklearn import metrics print(metrics.classification_report(y_test, y_pred, target_names=[&#39;Benign&#39;,&#39;Malignant&#39;])) . precision recall f1-score support Benign 0.98 0.93 0.95 43 Malignant 0.96 0.99 0.97 71 accuracy 0.96 114 macro avg 0.97 0.96 0.96 114 weighted avg 0.97 0.96 0.96 114 . print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;,metrics.precision_score(y_test, y_pred)) print(&quot;Recall:&quot;,metrics.recall_score(y_test, y_pred)) . Accuracy: 0.9649122807017544 Precision: 0.958904109589041 Recall: 0.9859154929577465 .",
            "url": "https://hakbilenberk.github.io/LearningML/2022/11/09/Logistic-Regression.ipynb.html",
            "relUrl": "/2022/11/09/Logistic-Regression.ipynb.html",
            "date": " ‚Ä¢ Nov 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Evaluation for Regression",
            "content": "Evaluation for Regression . Model evaluation is very important since we need to understand how well our model is performing. In comparison to classification, performance of a regression model is slightly harder to determine because, unlike classification, it is almost impossible to predict the exact value of a target variable. Therefore, we need a way to calculate how close our prediction value is to the real value. . There are different model evaluation metrics that are used popularly for regression models which we are going to dive into in the following sections. . Mean Absolute Error . Mean absolute error is a very intuitive and simple technique, therefore also popular. It is basically the average of the distances between the predicted and the true values. Basically the distances between the predicted and the real values are also the error terms. The overall error for the whole data is the average of all prediction error terms. We take the absolute of the distances/errors to prevent negative and positive terms/errors from cancelling each other. . $$ MAE = frac{1}{N} sum limits_{i=1}^{N}{|y_i - ≈∑_i|} $$ . Advantages . MAE is not sensitive to outliers. Use MAE when you do not want outliers to play a big role in error calculated. | . Disadvantages . MAE is not differentiable globally. This is not convenient when we use it as a loss function, due to the gradient optimization method. | . Mean Squared Error (MSE) . MSE is one of widely used metrics for regression problems. MSE is the the measure of average of squared distance between the actual values and the predicted values. Squared terms help to also take into consideration of negative terms and avoid cancellation of the total error between positive and negative differences. . $$ MSE = frac{1}{N} sum limits_{i=1}^{N}{(y_i - ≈∑_i)^2} $$ . Advantages . Graph of MSE is differantiable which means it can be easily used as a loss function. | MSE can be decomposed into variance and bias squared. This helps us understand the effect of variance or bias in data to the overall error. | . $$ MSE(≈∑) = Var(≈∑) + (Bias(≈∑))^2 $$ . Disadvantages . The value calculated MSE has a different unit than the target variable since it is squared. (Ex. meter --&gt; meter^2) | If there exists outliers in the data, then they are going to result in a larger error. Therefore, MSE is not robust to outliers (this can also be an advantage if you are looking to penalize outliers). | . Root Mean Squared Error (RMSE) . As the name already suggests, in RMSE we take the root of the mean of squared distances, meaning the root of MSE. RMSE is also a popularly used evaluation metric, especially in deep learning techniques. . $$ RMSE = sqrt{ frac{1}{N} sum limits_{i=1}^{N}{(y_i - ≈∑_i)^2} }$$ . Advantages . The error calculated has the same unit as the target variables making the interpretation relatively easier. | . Disadvantages . Just like MSE, RMSE is also susceptible to outliers. | . R-Squared . R square is a different metric compared to the ones we have discussed until now. It does not directly measure the error of the model. . R-squared evaluates the scatter of the data points around the fitted regression line. It is the percentage of the target variable variation which the model considers compared to the actual target variable variance. It is also known as the &quot;coefficient of determination&quot; or goodness of fit. . $$ R^2 = frac{ text{Variance considered by model}}{ text{Total variance}} $$ . $$ R^2 = 1 - frac{SS_{regression}}{SS_{total}} = 1 - frac{ sum{(y_i - ≈∑_i)^2}}{ sum{(y_i - y_{mean})^2}} $$ . As we can see above, R-squared is calculated by dividing the sum of squared error of predictions by the total sum of square, where predicted value is replaced by the mean of real values. . R-squared is always between 0 and 1. 0 indicates that the model does not explain any of the variation in the target variable around its mean value. The regression model basically predicts the mean of the parget variable. A value of 1 indicates, that the model explains all the variance in the target variable around its mean. . A larger R-squared value usually indicates that the regression model fits the data better. However, a high R-square model does not necessarily mean a good model. . import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression import seaborn as sns; sns.set_theme(color_codes=True) X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 15, random_state=42) plt.figure() ax = sns.regplot(x=X,y=y) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) . R-squared score: 0.977 . X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 200, random_state=42) plt.figure() ax = sns.regplot(x=X,y=y) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) . R-squared score: 0.294 . Advantages . R-square is a handy, and an intuitive metric of how well the model fits the data. Therefore, it is a good metric for a baseline model evaluation. However, due to the disadvantages we are going to discuss now, it should be used carefully. | . Disadvantages . R-squared can&#39;t determine if the predictions are biased, that is why looking at the residual plots in addition is a good idea. . | R-squared does not necessarily indicate that a regression model is good to go. It is also possible to have a low R-squared score for a good regression model and a high R-squared model for a bad model (especially due to overfitting). . | When new input variables (predictors) are added to the model, the R-square is going to increase (because we are adding more variance to the data) independent of an actual performance increase in model. It never decreases when new input variables are added. Therefore, a model with many input variables may seem to have a better performance jsut because it has more input variables. This is an issue which we are going to address with adjusted R-squared. . | . It is still possible to fit a good model to a dataset with a lot of variance which is likely going to have a low R-square. However, it does not necessarily mean the model is bad if it is still able to capture the general trend in the dataset, and capture the effect of change of a predictor on the target variables. R-square becomes a big problem when we want to predict a target variable with a high precision, meaning with a small prediction interval. . A high R-squared score also does not necessarily mean a good model because it is not able to detect the bias. Therefore, also checking the residual plots is a good idea. Like we mentioned previously, a model with a high R-squared score can also be overfitting since it captures most of the variance in the data. Therefore, it is always a good idea to check the R-squared score of the predictions from the model and compare it to the R-squared score from the training data. . Adjusted R-Squared . We previously mentioned that R-square score never actually decreases but increases when we add more input variables because we increase the variance in the data. To address this issue, we are going to talk about adjusted R-squared. . The adjusted R-sqaured is adjusted version of R-square where the number of input variables in the model are also considered. R-square can penalize the additional input variables given they do not contribute to the model performance. Let&#39;s have a look at how it is calculated. . $$ R_a^2 = 1- (( frac{n-1}{n-k-1})(1-R^2)) $$ . where: . $n = text{number of data points}$, $k= text{number of input variables}$ . As number of input variables/features increase, the denominator will decrease, R-squared will increase slightly or remain constant if the added features are not relevant, meaning the complete term in paranthesis is going to increase. The resultant adjusted R-squared score is going to decrease because we deducted the increasing term from 1. . However, if the added input variables are relevant, then the R-squared score will increase much more and the term (1-R2) will decrease a lot. When we subtract the complete term in paranthesis from 1, the overall adjusted R-squared score will increase. You see how the number of added variables and the amount of R-squared increase, help us adjust the R-squared score to account for new variables depending on if they are relevant or not. . def get_adj_r2(X,y): return 1 - (1-model.score(X, y))*(len(X) - 1) / (len(X) - (X.shape[1] - 1) - 1) . X, y = make_regression(n_samples = 80, n_features=4, n_informative=1, bias = 50, noise = 60, random_state=42) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) print(&#39;Adjusted R-squared score: {:.3f}&#39; .format(get_adj_r2(X,y))) . R-squared score: 0.582 Adjusted R-squared score: 0.566 . X, y = make_regression(n_samples = 80, n_features=4, n_informative=1, bias = 50, noise = 60, random_state=42) model = LinearRegression() model.fit(X, y) print(&#39;R-squared score: {:.3f}&#39; .format(model.score(X, y))) print(&#39;Adjusted R-squared score: {:.3f}&#39; .format(get_adj_r2(X,y))) . R-squared score: 0.582 Adjusted R-squared score: 0.566 .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/09/10/Evaluation-Regression.ipynb.html",
            "relUrl": "/jupyter/2022/09/10/Evaluation-Regression.ipynb.html",
            "date": " ‚Ä¢ Sep 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Linear Regression in Machine Learning",
            "content": "Content prepared by: Berk Hakbilen . What is Regression? . In statistical modeling, regression analysis estimates the relationship between one or more independent variables and the dependent variable which represents the outcome. . To explain with an example you can imagine a list of houses, with information regarding to the size, distance to city center, garden (independent variables). Using these information, you can try to understand how the price(dependent variables) changes. . So for a regression analysis we have a set of observations or samples with one or more variables/features. Then, we define a dependent variable (the outcome) and try to find a relation between the dependent variables and the independent variables. The best way to do this is by finding a function which best represent the data. . Linear Models for Regression . Linear Regression . In the linear regression model, we will use regression analysis to best represent the dataset through a linear function. Then, we will use this function to predict the outcome of a new sample/observation which was not in the dataset. . Linear regression is one of the most used regression models due to its simplicity and ease of understanding the results. Let&#39;s move on to the model formulations to understand this better. . . The linear regression function is written assuming a linear relationship between the variables: . $$y = w_1x_1 + ... + w_nx_n + c$$ . where w terms are the regression coefficients, x terms are the independent variables or features, y is dependent variable/outcome and c is the constant bias term. . We can write a simple linear regression function for the houses examples we mentioned above. . $$y_{price} = 500X_{size} - 350X_{distance to city} + 400.000$$ So if we plug in the features of a new house into this function, we can predict its price (let&#39;s assume size is 150m2 and distance to city center is 5 km). $$y_{price} = 500*150 - 350*5 + 400.000 = 75.000 - 1750 + 400.000 = 476.750$$ . See how the coefficient of distance to city center is minus. Meaning closer to center, more expensive the house will be. . We can create a simple fake regression dataset with only one feature and plot it to see the data behaviour more clearly. . import matplotlib.pyplot as plt from sklearn.datasets import make_regression plt.figure() plt.title(&#39;Samples in a dataset with only one feature (dependent variable)&#39;) X, y = make_regression(n_samples = 80, n_features=1, n_informative=1, bias = 50, noise = 40, random_state=42) plt.scatter(X, y, marker= &#39;o&#39;, s=50) plt.show() . The dataset above has only one dependent variable. In this case, the regression function would be: $$y = w_1x_1 + c$$ . where w1 would be the slope the curve and c would be the offset value. . When we train our model on this data, the coefficients and the bias term will be determined automatically so that the regression function best fits the dataset. . The model algorithm finds the best coefficients for the dataset by optimizing an objective function, which in this case would be the loss function. The loss function represents the difference between the predicted outcome values and the real outcome values. . Least-Squared Linear Regression . In the Least-Squared linear regression model the coefficients and bias are determined by minimizing the sum of squared differences (SSR) for all of the samples in the data. This model is also called Ordinary Least-Squares. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2}$$ . If we interpret the function, it is a function determined by taking the square of the difference between the predicted outcome value and the real outcome value. . Let&#39;s train the Linear Regression model using the fake dataset we previously created and have a look at the calculated coefficients. . from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42) model = LinearRegression() model.fit(X_train, y_train) print(&#39;feature coefficient (w_1): {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_train, y_train))) print(&#39;R-squared score (test): {:.3f}&#39; .format(model.score(X_test, y_test))) . feature coefficient (w_1): [101.41780398] intercept (c): 53.342 R-squared score (training): 0.891 R-squared score (test): 0.735 . Here, R^2 is the coefficient of determination. This term represents the amount of variation in outcome(y) explained by the dependence on features (x variables). Therefore, a larger R^2 indicates a better model performance or a better fit. . When R^2 is equal to one, then RSS is equals to 0. Meaning the predicted outcome values and the real outcome values are exactly the same. We will be using the R^2 term to measure the performance of our model. . plt.figure(figsize=(6,5)) plt.scatter(X, y, marker= &#39;o&#39;, s=50, alpha=0.7) plt.plot(X, model.coef_*X + model.intercept_, &#39;r-&#39;) plt.title(&#39;Least-squares linear regression model&#39;) plt.xlabel(&#39;Variable/feature value (x)&#39;) plt.ylabel(&#39;Outcome (y)&#39;) plt.show() . Ridge Regression - L2 Regularization . Ridge regression model calculates coefficients and the bias (w and c) using the same criteria in Least-Squared however with an extra term. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2} + boxed { alpha sum limits_{j=1}^{m} {w_j^2}}$$ . This term is a penalty to adjust the large variations in the coefficients. The linear prediction formula is still the same but only the way coefficients are calculated differs due to this extra penalty term. This is called regularization. It serves to prevent overfitting by restricting the variation of the coefficients which results in a less complex or simpler model. . This extra term is basically the sum of squares of the coefficients. Therefore, when we try to minimize the RSS function, we also minimize the the sum of squares of the coefficients which is called L2 regularization. Moreover, the alpha constant serves to control the influence of this regularization. This way, in comparison to the Least-Squared model, we can actually control the complexity of our model with the help of alpha term. The higher alpha term, higher the regularization is, and simpler the model will be. . The accuracy improvement with datasets including one dependent variable (feature) is not significant. However, for datasets with multiple features, regularization can be very effective to reduce model complexity, therefore overfitting and increase model performance on test set. . Let&#39;s have a look at its implementation in python. . from sklearn import datasets X,y = datasets.load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42) from sklearn.linear_model import Ridge model = Ridge() model.fit(X_train, y_train) print(&#39;feature coefficients: {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) . feature coefficients: [ 50.55155508 -67.72236516 278.3007281 197.62401363 -6.2459735 -26.22698465 -151.39451804 120.32319558 215.85469359 101.75554294] intercept (c): 152.514 . plt.figure(figsize=(10,5)) alphas = [1,5,10,20,50,100,500] features = [&#39;w_&#39;+str(i+1) for i,_ in enumerate(model.coef_)] for alpha in alphas: model = Ridge(alpha=alpha).fit(X_train,y_train) plt.scatter(features,model.coef_, alpha=0.7,label=(&#39;alpha=&#39;+str(alpha))) plt.axhline(0) plt.xticks(features) plt.legend(loc=&#39;upper left&#39;) plt.show() . Normalization can be applied unfairly to the features when they have different scales (when one feature has values around 0-1 and the other has from 100-1000). This can cause inaccuracies in our model when we apply regularization. In this case, feature scaling comes to our help to normalize all the values in the dataset, so that we can get rid of the scale differences. We will look in to feature scaling in another section... . Lasso Regression - L1 Regularization . Lasso regression is also a regularized linear regression model. In comparison to Ridge regression, it uses L1 regularization as the penalty term while calculating the coefficients. . Let&#39;s have a look at how the RSS function looks like with the penalty term for L1 regularization. . $$RSS( textbf w,c) = sum limits_{i=1}^{n} {( textbf y_i - ( textbf w. textbf x_i + b))^2} + boxed { alpha sum limits_{j=1}^{m} {|w_j|}}$$ . The penalty term for L1 regularization is the sum of absolute values of the coefficients. Therefore, when the algorithm tries to minimize RSS, it enforces the regularization by minimizing the sum of absolute values of the coefficients. . This results in coefficients of the least effective paramaters to be 0 which is kind of like feature selection. Therefore, it is most effectively used for datasets where there a few features with a more dominant effect compared to others. This results in eliminating features which have a small effect by setting their coefficients to 0. . Alpha term is again used to control the amount of regularization. . from sklearn.linear_model import Lasso model = Lasso() model.fit(X_train, y_train) print(&#39;feature coefficients: {}&#39; .format(model.coef_)) print(&#39;intercept (c): {:.3f}&#39; .format(model.intercept_)) . feature coefficients: [ 0. -0. 398.38436775 46.17884277 0. 0. -0. 0. 238.18740159 0. ] intercept (c): 152.944 . After finding the coefficients of the dominant features, we can go ahead and list their labels. . import numpy as np data = datasets.load_diabetes() np.take(data.feature_names,np.nonzero(model.coef_)) . array([[&#39;bmi&#39;, &#39;bp&#39;, &#39;s5&#39;]], dtype=&#39;&lt;U3&#39;) . alphas = [0.1,0.5,1,2,5,10] for alpha in alphas: model = Lasso(alpha=alpha).fit(X_train,y_train) print(&#39;feature coefficients for alpha={}: n{}&#39;.format(alpha,model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_train, y_train))) print(&#39;R-squared score (test): {:.3f} n&#39; .format(model.score(X_test, y_test))) . feature coefficients for alpha=0.1: [ 0. -151.41550421 540.85933829 337.22472854 -85.19030645 -0. -262.90345036 0. 418.24822392 9.92430726] R-squared score (training): 0.508 R-squared score (test): 0.502 feature coefficients for alpha=0.5: [ 0. -0. 492.28522027 187.79307581 -0. -0. -107.63391392 0. 322.2269439 0. ] R-squared score (training): 0.451 R-squared score (test): 0.477 feature coefficients for alpha=1: [ 0. -0. 398.38436775 46.17884277 0. 0. -0. 0. 238.18740159 0. ] R-squared score (training): 0.347 R-squared score (test): 0.379 feature coefficients for alpha=2: [ 0. 0. 75.24915126 0. 0. 0. -0. 0. 0. 0. ] R-squared score (training): 0.052 R-squared score (test): 0.041 feature coefficients for alpha=5: [ 0. 0. 0. 0. 0. 0. -0. 0. 0. 0.] R-squared score (training): 0.000 R-squared score (test): -0.014 feature coefficients for alpha=10: [ 0. 0. 0. 0. 0. 0. -0. 0. 0. 0.] R-squared score (training): 0.000 R-squared score (test): -0.014 . Ridge or Lasso? . To sum up, it makes sense to use the Ridge regression model there are many small to medium effective features. If there are only a few dominantly effective features, use the Lasso regression model. . Polynomial Regression . Linear regression performs well on the assumption that the relationship between the independent variables (features) and the dependent variable(outcome) is linear. If the distrubtion of the data is more complex and does not show a linear behaviour, can we still use linear models to represent such datasets? This is where polynomial regression comes in very useful. . To capture this complex behaviour, we can add higher order terms to represent the features in the data. Transforming the linear model with one feature: . $$y = w_1x + c rightarrow boxed{y=w_1x + w_2x^2 + c} $$ . Since the coefficients are related to features linearly, this is still a liner model. However, it contains quadratic terms and the curve fitted is a polynomial curve. . Let&#39;s continue with an example for Polynomial regression. To convert the features to higher order terms, we can use the PolynomialFeatures class from scikit-learn. Then we can use the Linear regression model from before to train the model. . But before, let us create a dataset which could be a good fit for a 2nd degree function. For that we will use numpy to create random X points and plug them into a representative function. . np.random.seed(0) X = 2 - 3 * np.random.normal(0, 1, 100) y = X - 2 * (X ** 2) + np.random.normal(-3, 3, 100) plt.scatter(X, y, s=10) plt.show() . We can reshape the arrays we created so that we can feed them in to the model. First, we will train a LinearRegression model to see how it fits to this data. . X = X[:, np.newaxis] y = y[:, np.newaxis] model = LinearRegression() model.fit(X,y) print(&#39;feature coefficients: n{}&#39;.format(model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X, y))) plt.plot(X, model.coef_*X + model.intercept_, &#39;r-&#39;) plt.scatter(X,y, s=10) plt.show() . feature coefficients: [[-6.36572661]] R-squared score (training): 0.397 . As expected, Linear Regression model does not provide a very good fit with the normal features for a dataset of this behaviour. Now, we can create 2nd order Polynomial features using the PolynomialFeatures class from sk-learn library. Then, use these new 2nd order features to train the same linear regression model. . from sklearn.preprocessing import PolynomialFeatures poly_features= PolynomialFeatures(degree=2) X_poly = poly_features.fit_transform(X) model = LinearRegression() model.fit(X_poly,y) print(&#39;feature coefficients: n{}&#39;.format(model.coef_)) print(&#39;R-squared score (training): {:.3f}&#39; .format(model.score(X_poly, y))) plt.scatter(X,model.predict(X_poly),s=10,label=&quot;polynomial prediction&quot;) plt.scatter(X,y,s=10,label=&quot;real data&quot;) plt.legend(loc=&#39;lower left&#39;) plt.show() . feature coefficients: [[ 0. 0.96597113 -2.02225052]] R-squared score (training): 0.990 . This time, we were able to obtain a very good fit using the same linear regression model but with 2nd order features obtained from the PolynomialFeatures class. This is a perfect example to show how Polynomial Linear Regression can be used to obtain better fits with data which do not have a linear relationship between the features and the outcome value. .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2022/03/19/Linear-Regression.ipynb.html",
            "relUrl": "/jupyter/2022/03/19/Linear-Regression.ipynb.html",
            "date": " ‚Ä¢ Mar 19, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://hakbilenberk.github.io/LearningML/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://hakbilenberk.github.io/LearningML/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey thereüëã My name is Berk Hakbilen. I am currently working as a data scientist. I transitioned into a career into data science after discovering my passion for data and developing myself in the field of data science as a hobby. During this transition, free online material available was a huge help for me. That‚Äôs why I decided to create this blog, to publish some of the free learning material that I have created in my free time, so that it can be helpful for others who are interested about data science! . I hope this material is useful for you. If you have any questions or feedback, feel free to contact me! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://hakbilenberk.github.io/LearningML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://hakbilenberk.github.io/LearningML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}